<!DOCTYPE HTML>
<html>
	<head>




	<title>Publications | LiaRom | Learning to Interpret, Automate, and Rely on Machines</title>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-45108170-4"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-45108170-4');
	</script>
	<script type="text/javascript" async
  		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<meta name="description" content="Learning to Interpret, Automte, and Rely on Machines (LiaRom) is Peng Wang's (peng.wang@sheffield.ac.uk) personal website. He is currently working as a research associate on various subjects. Specifically, he is dedicated to provide reliable solutions to safety critical systems, such as autonomous driving, human-robot interaction, etc." />
	<meta name="keywords" content="Publications | LiaRom | Learning to Interpret, Automte, and Rely on Machines, LiaRom, Sheffield, Autonomous Vehicles, AV, Self-Loclisation, SL, Artificial Intelligence, AI, Gaussian Process, GP, Deep Learning, DL, Bayesian Inference, BI, Uncertainty Quantification, UQ, Peng, Wang, Peng Wang, peng.wang" />
	<meta charset="utf-8" />
	<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
	<meta http-equiv="Pragma" content="no-cache" />
	<meta http-equiv="Expires" content="0" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<link rel="stylesheet" href="/assets/css/main.css" />

    <link rel="apple-touch-icon-precomposed" href="https://oatml.cs.ox.ac.uk/images/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" href="https://oatml.cs.ox.ac.uk/images/apple-touch-icon-114x114.png">
    <link rel="shortcut icon" href="https://oatml.cs.ox.ac.uk/images/favicon.ico">

	<meta property="og:title" content="Publications | LiaRom | Learning to Interpret, Automate, and Rely on Machines" />
	<meta property="og:type" content="website" />
	<meta property="og:image" content="https://grapesonwang.github.io/images/coast.jpg" />
	<meta property="og:description" content="Learning to Interpret, Automte, and Rely on Machines (LiaRom) is Peng Wang's (peng.wang@sheffield.ac.uk) personal website. He is currently working as a research associate on various subjects. Specifically, he is dedicated to provide reliable solutions to safety critical systems, such as autonomous driving, human-robot interaction, etc." />
	<style type="text/css">
		.my_headline {
			color: inherit !important;
			font-weight: inherit;
			text-decoration: inherit;
		}
		/*.my_logo {display: block !important; margin: auto; padding: 20px 0px 20px 0px; width: 50%;}*/
		.my_logo {
			display: block !important;
			margin: auto;
			padding: 20px 0px 20px 0px;
			max-height: 70%;
		}
		.banner > article .inner {height: 100%}
		.banner > article .inner > header {height: 85%}
		.banner_top {height: 15%}
		body.is-mobile .banner_top {height: 5%}
		@media screen and (max-width: 980px) {
			.banner_top {height: 10%}
			.my_logo {max-height: 50%}
		}
		.my_box_image {overflow: hidden; max-height: 50px;}
		.my_news_div {padding: 0 !important}
		.my_news_div div {margin: 0em 1em 1em 1em}
		.my_news_div div header h4 {
			font-weight: 700;
			text-transform: uppercase;
			color: #484848 !important;
			word-spacing: 3pt;
		}
		.my_news_div div header p {
			font-size: 0.9em;
			color: #aaa !important;
			margin-top: -0.6em;
			margin-bottom: 0.1em;
		}
		.my_news_span {
			padding: 0 !important;
			background-color: transparent !important;
		}
		.my_news_span img {
			border-radius: 100%;
			width: 100%;
			border: solid 0.5em rgba(144, 144, 144, 0.25);
		}
		div #my_div_grid:nth-child(2n) {
			background-color: rgba(0, 0, 0, 0.075);
		}
		div #my_div_grid {
			padding: 1rem 0rem 1rem 0rem;
			margin: 0;
		}
		.wrapper.style3.pageheader {
			background-size: 100% !important;
			background-position: top !important;
			background-image: url('https://oatml.cs.ox.ac.uk/images/bg_crop.jpg') !important;
		}
		/* Mobile screens */
		@media screen and (max-width: 980px) {
			.wrapper.style3.pageheader {background-size: 200% !important;}
			.ox-link {display: none;}
			.group-title-link {display: none;}
			.group-title-link-short {display: inline;}
			div.my_news_div div.fit {
				text-align : inherit !important;
			}
			span.my_news_span.image.right {
				float : left;
				margin: 0 1.5rem 1rem 0;
			}
		}
		/* Mobile screens */
		@media screen and (max-width: 435px) {
			.my_header {
				min-height: 120pt !important;
			}
		}
		/* Desktop screens */
		@media screen and (min-width: 980px) {
			.group-title-link-short {display: none;}
			.group-title-link {
				display: inline;
				vertical-align: bottom !important;
				border-left: 2px solid white;
				height: 500px;
				padding: 0px 0px 0px 10px;
				margin: 0px 0px 0px 10px;
			}
		}
		/* Mobile screens */
		@media screen and (max-width: 980px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('/images/tree.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('/images/tree_1.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('/images/tower.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('/images/sea.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('/images/rock.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('/images/island.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('/images/cross.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('/images/coast.jpg') !important;
				}
			
		}
		/* Desktop screens */
		@media screen and (min-width: 980px) and (max-width: 1920px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('/images/tree.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('/images/tree_1.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('/images/tower.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('/images/sea.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('/images/rock.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('/images/island.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('/images/cross.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('/images/coast.jpg') !important;
				}
			
		}
		/* Huge screens */
		@media screen and (min-width: 1920px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('/images/tree.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('/images/tree_1.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('/images/tower.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('/images/sea.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('/images/rock.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('/images/island.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('/images/cross.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('/images/coast.jpg') !important;
				}
			
		}
		/* Mobile screens */
		@media screen and (max-height: 600px) {
			.indicators {display: none;}
		}
	</style>

</head>
<body>

<!-- Header -->
			<header id="header" class="alt">
				<div class="logo">
					<img src="/images/TUOS_logo.png" class="ox-link" style="height: 58px; vertical-align: top !important" alt="The University of Sheffield Department of ACSE" usemap="#compscimap" />
					<map name="compscimap" id="compscimap">
                                <area shape="rect" coords="0,0,55,55" href="http://www.sheffield.ac.uk/" alt="The University of Sheffield" title="The University of Sheffield"/>
                                <area shape="rect" coords="65,0,170,55" href="https://www.sheffield.ac.uk/acse" alt="Department of Automatic Control and Systems Engineering - Home" title="ACSE - Home"/>
                    </map>
					<a href="https://www.sheffield.ac.uk/acse/department/people/research-staff/p-wang" class="group-title-link">
						Home
					</a>

<!--					<a href="https://grapesonwang.github.io/index.html" class="group-title-link-short">
						Local-Home
					</a>  -->

				</div>
					<a href="https://grapesonwang.github.io/index.html" class="group-title-link-short">
						LiaRom
					</a>

<!--				<a href="index.html#menu">LiaRom</a>   --> 
			</header>



<!-- Nav
			<nav id="menu">
				<ul class="links">
					<li><a href="https://grapesonwang.github.io/index.html">Home</a></li>
					<li><a href="news.html">News</a></li>
					<li><a href="publications.html">Publications</a></li>
					<li><a href="blog.html">Blog</a></li>
				</ul>
			</nav>
 -->


		<!-- One -->
					<section id="One" class="wrapper style3 pageheader">
				<div class="inner">
					<header class="align-center">
						<p>Learning to Interpret, Automate, and Rely on Machines</p>
						<h1>LiaRom</h1>
					</header>
				</div>
			</section>

		<!-- Two -->
			<section id="two" class="wrapper style2">
				<div class="inner">
					<div class="box">
						<div class="content">
							<header id="title" class="align-center">
								<p>Publications</p>
								<!-- <h2>Lorem ipsum dolor</h2> -->
							</header>
							

<!-- Showing publications: publications -->


<style>
#greyImage {
  filter: gray; /* IE6-9 */
  -webkit-filter: grayscale(1); /* Google Chrome, Safari 6+ & Opera 15+ */
  filter: grayscale(1); /* Microsoft Edge and Firefox 35+ */
}
</style>

<div class="box alt">
  <div class="row 50% uniform">
    <div class="2u 6u(small)"><span class="image fit" style="text-align: center;">
        
          <img id="greyImage" src="https://oatml.cs.ox.ac.uk/images/science.png" alt="" />All publications
        
    </span></div>
    <div class="2u 6u$(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/BDL.html#title"><img src="images/uncertainty_fig-square.jpg" alt="" />Bayesian Deep Learning</a>
        
    </span></div>
    <div class="2u 6u(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/deep_learing.html#title"><img src="images/neural_network-square.jpg" alt="" />Deep Learning</a>
        
    </span></div>
    <div class="2u 6u$(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/inference.html#title"><img src="https://oatml.cs.ox.ac.uk/images/Bayes_square.jpg" alt="" />Inference</a>
        
    </span></div>
    <div class="2u 6u(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/data_efficient.html#title"><img src="https://oatml.cs.ox.ac.uk/images/new20horizon_unicycle_2.png" alt="" />Data Efficient AI</a>
        
    </span></div>
    <div class="2u$ 6u$(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/adversarial_interpretability.html#title"><img src="https://oatml.cs.ox.ac.uk/images/saliency_masked.jpg" alt="" />Adversarial and Interpretable ML</a>
        
    </span></div>
    <!-- Break -->
    <div class="2u 6u(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/autonomous_driving.html#title"><img src="https://oatml.cs.ox.ac.uk/images/autonomous_driving_2.jpg" alt="" />Autonomous Driving</a>
        
    </span></div>
    <div class="2u 6u$(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/RL.html#title"><img src="images/virel.jpg" alt="" />Reinforcement Learning</a>
        
    </span></div>
    <div class="2u 6u(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/NLP.html#title"><img src="https://oatml.cs.ox.ac.uk/images/word-alignment.png" alt="" />Natural Language Processing</a>
        
    </span></div>
    <div class="2u 6u$(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/space.html#title"><img src="https://oatml.cs.ox.ac.uk/images/astro.jpg" alt="" />Space and Earth Observations</a>
        
    </span></div>
    <div class="2u 6u(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/medical.html#title"><img src="https://oatml.cs.ox.ac.uk/images/medical_2.jpg" alt="" />Medical</a>
        
    </span></div>
    <div class="2u$ 6u$(small)"><span class="image fit" style="text-align: center;">
        
          <a href="https://oatml.cs.ox.ac.uk/tags/AI_for_good_safety.html#title"><img src="images/un_talk.png" alt="" />AI for Good and AI safety</a>
        
    </span></div>
    <!-- Break -->
  </div>
</div>




<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Smith2020CapsulesWorkshop" style="text-align: center;">
    <span class="images">
      <a href="https://oolworkshop.github.io/program/ool_6.html" target="_blank">
        <img src="images/capsule_parts.png" title="Capsule Networks: A Generative Probabilistic Perspective" alt="Capsule Networks: A Generative Probabilistic Perspective" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Capsule Networks: A Generative Probabilistic Perspective</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Capsule Networks: A Generative Probabilistic Perspective</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>‘Capsule’ models try to explicitly represent the poses of objects, enforcing a linear relationship between an objects pose and those of its constituent parts. This modelling assumption should lead to robustness to viewpoint changes since the object-component relationships are invariant to the poses of the object. We describe a probabilistic generative model that encodes these assumptions. Our probabilistic formulation separates the generative assumptions of the model from the inference scheme, which we derive from a variational bound. We experimentally demonstrate the applicability of our unified objective, and the use of test time optimisation to solve problems inherent to amortised inference.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/lisa_schut/">Lisa Schut</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Mark van der Wilk
    <br>
    <i> Object Oriented Learning Workshop, ICML 2020</i> <br> [<a href="https://oolworkshop.github.io/program/ool_6.html" target="_blank">Paper</a>] <br>
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202007_Smith2020CapsulesWorkshop.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Zhang2020Invariant" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/2003.06016" target="_blank">
        <img src="images/virel.jpg" title="Invariant Causal Prediction for Block MDPs" alt="Invariant Causal Prediction for Block MDPs" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Invariant Causal Prediction for Block MDPs</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Invariant Causal Prediction for Block MDPs</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Generalization across environments is critical to the successful application of reinforcement learning algorithms to real-world challenges. In this paper, we consider the problem of learning abstractions that generalize in block MDPs, families of environments with a shared latent state space and dynamics structure over that latent space, but varying observations. We leverage tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting. We prove that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return. We further provide more general bounds on model error and generalization error in the multi-environment setting, in the process showing a connection between causal variable selection and the state abstraction framework for MDPs. We giv... [<a href="https://oatml.cs.ox.ac.uk/publications/202006_Zhang2020Invariant.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Amy Zhang, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, Shagun Sodhani, <a href="members/angelos_filos/index.html">Angelos Filos</a>, Marta Kwiatkowska, Joelle Pineau, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Doina Precup
    <br>
    <i>Causal Learning for Decision Making Workshop at ICLR, 2020</i> <br> [<a href="https://causalrlworkshop.github.io/pdf/CLDM_17.pdf" target="_blank">Paper</a>] <br> <b><i>ICML, 2020</i></b> <br> [<a href="https://arxiv.org/abs/2003.06016" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202006_Zhang2020Invariant.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="VanAmersfoortSmithGal2020Simple" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/2003.02037" target="_blank">
        <img src="images/twomoons.png" title="Uncertainty Estimation Using a Single Deep Deterministic Neural Network" alt="Uncertainty Estimation Using a Single Deep Deterministic Neural Network" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Uncertainty Estimation Using a Single Deep Deterministic Neural Network</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Uncertainty Estimation Using a Single Deep Deterministic Neural Network</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="members/lewis_smith/index.html">Lewis Smith</a>, Yee Whye Teh, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>ICML, 2020</b></i> <br> [<a href="https://arxiv.org/abs/2003.02037" target="_blank">Paper</a>] [<a href="bibtex/VanAmersfoortSmithGal2020Simple.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202006_VanAmersfoortSmithGal2020Simple.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Rudner2020Inter" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1912.04242" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/uncertainty_small.jpg" title="Inter-domain Deep Gaussian Processes with RKHS Fourier Features" alt="Inter-domain Deep Gaussian Processes with RKHS Fourier Features" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Inter-domain Deep Gaussian Processes with RKHS Fourier Features</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Inter-domain Deep Gaussian Processes with RKHS Fourier Features</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Inter-domain Gaussian processes (GPs) allow for high flexibility and low computational cost when performing approximate inference in GP models. They are particularly suitable for modeling data exhibiting global function behavior but are limited to stationary covariance functions and thus fail to model non-stationary data effectively. We propose Inter-domain Deep Gaussian Processes with RKHS Fourier Features, an extension of shallow inter-domain GPs that combines the advantages of inter-domain and deep Gaussian processes (DGPs) and demonstrate how to leverage existing approximate inference approaches to perform simple and scalable approximate inference on Inter-domain Deep Gaussian Processes. We assess the performance of our method on a wide range of prediction problems and demonstrate that it outperforms inter-domain GPs and DGPs on challenging large-scale and high-dimensional real-world datasets exhibiting both global behavior as well as a high-degree of non-stationarity.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, Dino Sejdinovic, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><i>ICML, 2020</i></b> <br> [<a href="https://icml.cc/Conferences/2020/AcceptedPapersInitial" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202006_Rudner2019Inter.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Filos2020Can" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/2006.14911" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/filos2020can.png" title="Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?" alt="Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called <em>robust imitative planning</em> (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model’s uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term <em>adaptive robust imitative planning</em> (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes <em>prediction</em> challenge, but since no be... [<a href="https://oatml.cs.ox.ac.uk/publications/202006_Filos2020Can.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/angelos_filos/index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><i>ICML, 2020</i></b> <br> [<a href="https://arxiv.org/abs/2006.14911" target="_blank">Paper</a>] [<a href="https://github.com/OATML/carsuite" target="_blank">Code</a>] [<a href="https://sites.google.com/view/av-detect-recover-adapt" target="_blank">Website</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202006_Filos2020Can.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Brauner2020effectiveness" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1912.04242" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/medical_2.jpg" title="The effectiveness and perceived burden of nonpharmaceutical interventions against COVID-19 transmission: a modelling study with 41 countries" alt="The effectiveness and perceived burden of nonpharmaceutical interventions against COVID-19 transmission: a modelling study with 41 countries" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>The effectiveness and perceived burden of nonpharmaceutical interventions against COVID-19 transmission: a modelling study with 41 countries</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>The effectiveness and perceived burden of nonpharmaceutical interventions against COVID-19 transmission: a modelling study with 41 countries</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Existing analyses of nonpharmaceutical interventions (NPIs) against COVID19 transmission have concentrated on the joint effectiveness of large-scale NPIs. With increasing data, we can move beyond estimating joint effects towards disentangling individual effects. In addition to effectiveness, policy decisions ought to account for the burden placed by different NPIs on the population. Methods: To our knowledge, this is the largest data-driven study of NPI effectiveness to date. We collected chronological data on 9 NPIs in 41 countries between January and April 2020, using extensive fact-checking to ensure high data quality. We infer NPI effectiveness with a novel semi-mechanistic Bayesian hierarchical model, modelling both confirmed cases and deaths to increase the signal from which NPI effects can be inferred. Finally, we study how much perceived burden different NPIs impose on the population with an online survey of preferences using the MaxDiff method. Results: Eight NPIs have ... [<a href="https://oatml.cs.ox.ac.uk/publications/202006_Brauner2020effectiveness.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Jan Markus Brauner, <a href="https://oatml.cs.ox.ac.uk/members/soren_mindermann/">Sören Mindermann</a>, Mrinank Sharma, Anna B Stephenson, Tomáš Gavenčiak, David Johnston, John Salvatier, Gavin Leech, Tamay Besiroglu, George Altman, Hong Ge, Vladimir Mikulik, Meghan Hartwick, Yee Whye Teh, Leonid Chindelevitch, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Jan Kulveit
    <br>
    <i>Under review</i> <br> [<a href="https://doi.org/10.1101/2020.05.28.20116129" target="_blank">Preprint</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202006_Brauner2020effectiveness.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Zhou2020Divide" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1910.13324" target="_blank">
        <img src="images/neural_network.jpg" title="Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support" alt="Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Universal probabilistic programming systems (PPSs) provide a powerful framework for specifying rich and complex probabilistic models. They further attempt to automate the process of drawing inferences from these models, but doing this successfully is severely hampered by the wide range of non–standard models they can express. As a result, although one can specify complex models in a universal PPS, the provided inference engines often fall far short of what is required. In particular, we show they produce surprisingly unsatisfactory performance for models where the support may vary between executions, often doing no better than importance sampling from the prior. To address this, we introduce a new inference framework: Divide, Conquer, and Combine, which remains efficient for such models, and show how it can be implemented as an automated and general-purpose PPS inference engine. We empirically demonstrate substantial performance improvements over existing approaches on two examp... [<a href="https://oatml.cs.ox.ac.uk/publications/202005_Zhou2020Divide.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Yuan Zhou, Hongseok Yang, Yee Whye Teh, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>
    <br>
    <b><i>ICML, 2020</i></b> <br> [<a href="https://arxiv.org/abs/1910.13324" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202005_Zhou2020Divide.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Ruswurm2020Model" style="text-align: center;">
    <span class="images">
      <a href="https://igarss2020.org/" target="_blank">
        <img src="images/esa_talk.jpg" title="Model And Data Uncertainty For Satellite Time Series Forecasting With Deep Recurrent Models" alt="Model And Data Uncertainty For Satellite Time Series Forecasting With Deep Recurrent Models" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Model And Data Uncertainty For Satellite Time Series Forecasting With Deep Recurrent Models</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Model And Data Uncertainty For Satellite Time Series Forecasting With Deep Recurrent Models</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Deep Learning is often criticized as black-box method which often provides accurate predictions, but limited explanation of the underlying processes and no indication when to not trust those predictions. Equipping existing deep learning models with an (approximate) notion of uncertainty can help mitigate both these issues therefore their use should be known more broadly in the community. The Bayesian deep learning community has developed model-agnostic and easyto-implement methodology to estimate both data and model uncertainty within deep learning models which is hardly applied in the remote sensing community. In this work, we adopt this methodology for deep recurrent satellite time series forecasting, and test its assumptions on data and model uncertainty. We demonstrate its effectiveness on two applications on climate change, and event change detection and outline limitations.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Marc Rußwurm, Syed Mohsin Ali, Xiao Xiang Zhu, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Marco Körner
    <br>
    <b><span style="color:red">Student Paper Competition Finalists (out of 250 submissions)</span>, <i>IGARSS 2020</i></b> <br> [Paper]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202005_Ruswurm2020Model.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Mehta2020Uncertainty" style="text-align: center;">
    <span class="images">
      <a href="https://openreview.net/forum?id=H-PvDNIex" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/medical_2.jpg" title="Uncertainty Evaluation Metric for Brain Tumour Segmentation" alt="Uncertainty Evaluation Metric for Brain Tumour Segmentation" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Uncertainty Evaluation Metric for Brain Tumour Segmentation</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Uncertainty Evaluation Metric for Brain Tumour Segmentation</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>In this paper, we develop a metric designed to assess and rank uncertainty measures for the task of brain tumour sub-tissue segmentation in the BraTS 2019 sub-challenge on uncertainty quantification. The metric is designed to: (1) reward uncertainty measures where high confidence is assigned to correct assertions, and where incorrect assertions are assigned low confidence and (2) penalize measures that have higher percentages of under-confident correct assertions. Here, the workings of the components of the metric are explored based on a number of popular uncertainty measures evaluated on the BraTS 2019 dataset.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Raghav Mehta, <a href="members/angelos_filos/index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Tal Arbel
    <br>
    <b><i>MIDL, 2020</i></b> <br> [<a href="https://openreview.net/forum?id=H-PvDNIex" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202005_Mehta2020Uncertainty.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="MichelmoreGal2020Uncertainty" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1909.09884" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/cv_uncertainty.jpg" title="Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control" alt="Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Uncertainty Quantification with Statistical Guarantees in End-to-End Autonomous Driving Control</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Deep neural network controllers for autonomous driving have recently benefited from significant performance improvements, and have begun deployment in the real world. Prior to their widespread adoption, safety guarantees are needed on the controller behaviour that properly take account of the uncertainty within the model as well as sensor noise. Bayesian neural networks, which assume a prior over the weights, have been shown capable of producing such uncertainty measures, but properties surrounding their safety have not yet been quantified for use in autonomous driving scenarios. In this paper, we develop a framework based on a state-of-the-art simulator for evaluating end-to-end Bayesian controllers. In addition to computing pointwise uncertainty measures that can be computed in real time and with statistical guarantees, we also provide a method for estimating the probability that, given a scenario, the controller keeps the car safe within a finite horizon. We experimentally ev... [<a href="https://oatml.cs.ox.ac.uk/publications/202001_MichelmoreGal2020Uncertainty.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Rhiannon Michelmore, Matthew Wicker, Luca Laurenti, Luca Cardelli, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Marta Kwiatkowska
    <br>
    <i><b>2020 International Conference on Robotics and Automation (ICRA)</b></i> <br> [<a href="https://arxiv.org/abs/1909.09884" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202001_MichelmoreGal2020Uncertainty.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Farquhar2020Try" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/2002.03704" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/try_depth.png" title="Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks" alt="Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We challenge the longstanding assumption that the mean-field approximation for variational inference in Bayesian neural networks is severely restrictive. We argue mathematically that full-covariance approximations only improve the ELBO if they improve the expected log-likelihood. We further show that deeper mean-field networks are able to express predictive distributions approximately equivalent to shallower full-covariance networks. We validate these observations empirically, demonstrating that deeper models decrease the divergence between diagonal- and full-covariance Gaussian fits to the true posterior.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="members/lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><span style="color:red">Contributed talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/45.pdf" target="_blank">Workshop paper</a>], [<a href="https://arxiv.org/abs/2002.03704" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202001_Farquhar2020Try.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Farquhar2020Radial" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1907.00865" target="_blank">
        <img src="images/neural_network.jpg" title="Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning" alt="Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We propose Radial Bayesian Neural Networks (BNNs): a variational approximate posterior for BNNs which scales well to large models while maintaining a distribution over weight-space with full support. Other scalable Bayesian deep learning methods, like MC dropout or deep ensembles, have discrete support—they assign zero probability to almost all of the weight-space. Unlike these discrete support methods, Radial BNNs’ full support makes them suitable for use as a prior for sequential inference. In addition, they solve the conceptual challenges with the a priori implausibility of weight distributions with discrete support. The Radial BNN is motivated by avoiding a sampling problem in ‘mean-field’ variational inference (MFVI) caused by the so-called ‘soap-bubble’ pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are robust to hyperparameters and can be efficiently applied to a challenging real-world medical application without needing ad-hoc tweaks and inte... [<a href="https://oatml.cs.ox.ac.uk/publications/202001_Farquhar2020Radial.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/sebastian_farquhar/index.html">Sebastian Farquhar</a>, Michael Osborne, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)</b></i> <br> [<a href="https://arxiv.org/abs/1907.00865" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202001_Farquhar2020Radial.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Alizadeh2020Gradient" style="text-align: center;">
    <span class="images">
      <a href="https://openreview.net/forum?id=ryxK0JBtPr" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/alizadeh2020gradient_cross_section.png" title="Gradient \(\ell_1\) Regularization for Quantization Robustness" alt="Gradient \(\ell_1\) Regularization for Quantization Robustness" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Gradient \(\ell_1\) Regularization for Quantization Robustness</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Gradient \(\ell_1\) Regularization for Quantization Robustness</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We analyze the effect of quantizing weights and activations of neural networks on their loss and derive a simple regularization scheme that improves robustness against post-training quantization. By training quantization-ready networks, our approach enables storing a single set of weights that can be quantized on-demand to different bit-widths as energy and memory requirements of the application change. Unlike quantization-aware training using the straight-through estimator that only targets a specific bit-width and requires access to training data and pipeline, our regularization-based method paves the way for ``on the fly’’ post-training quantization to various bit-widths. We show that by modeling quantization as a <script type="math/tex">\ell_\infty</script>-bounded perturbation, the first-order term in the loss expansion can be regularized using the <script type="math/tex">\ell_1</script>-norm of gradients. We experimentally validate our method on different architectures on ... [<a href="https://oatml.cs.ox.ac.uk/publications/202001_Alizadeh2020Gradient.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen Blankevoort, Max Welling
    <br>
    <i><b>ICLR, 2020</b></i> <br> [<a href="https://openreview.net/forum?id=ryxK0JBtPr" target="_blank">OpenReview</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/202001_Alizadeh2020Gradient.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Zintgraf2020VariBAD" style="text-align: center;">
    <span class="images">
      <a href="https://openreview.net/forum?id=Hkl9JlBYvr&noteId=Hkl9JlBYvr" target="_blank">
        <img src="images/neural_network.jpg" title="VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning" alt="VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent’s uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We also evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher return during training than existing methods.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Katja Hofmann, Shimon Whiteson
    <br>
    <i><b>ICLR, 2020</b></i> <br> [<a href="https://openreview.net/forum?id=Hkl9JlBYvr&noteId=Hkl9JlBYvr" target="_blank">OpenReview</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Zintgraf2020VariBAD.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Ru2020BayesOpt" style="text-align: center;">
    <span class="images">
      <a href="https://openreview.net/forum?id=Hkem-lrtvH" target="_blank">
        <img src="images/neural_network.jpg" title="BayesOpt Adversarial Attack" alt="BayesOpt Adversarial Attack" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>BayesOpt Adversarial Attack</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>BayesOpt Adversarial Attack</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Black-box adversarial attacks require a large number of attempts before finding successful adversarial examples that are visually indistinguishable from the original input. Current approaches relying on substitute model training, gradient estimation or genetic algorithms often require an excessive number of queries. Therefore, they are not suitable for real-world systems where the maximum query number is limited due to cost. We propose a query-efficient black-box attack which uses Bayesian optimisation in combination with Bayesian model selection to optimise over the adversarial perturbation and the optimal degree of search space dimension reduction. We demonstrate empirically that our method can achieve comparable success rates with 2-5 times fewer queries compared to previous state-of-the-art black-box attacks.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Binxin Ru, Adam Cobb, Arno Blaas, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>ICLR, 2020</b></i> <br> [<a href="https://openreview.net/forum?id=Hkem-lrtvH" target="_blank">OpenReview</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Ru2020BayesOpt.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Salvatelli2019Using" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1911.04006" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astro.jpg" title="Using U-Nets to Create High-Fidelity Virtual Observations of the Solar Corona" alt="Using U-Nets to Create High-Fidelity Virtual Observations of the Solar Corona" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Using U-Nets to Create High-Fidelity Virtual Observations of the Solar Corona</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Using U-Nets to Create High-Fidelity Virtual Observations of the Solar Corona</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Understanding and monitoring the complex and dynamic processes of the Sun is important for a number of human activities on Earth and in space. For this reason, NASA’s Solar Dynamics Observatory (SDO) has been continuously monitoring the multi-layered Sun’s atmosphere in high-resolution since its launch in 2010, generating terabytes of observational data every day. The synergy between machine learning and this enormous amount of data has the potential, still largely unexploited, to advance our understanding of the Sun and extend the capabilities of heliophysics missions. In the present work, we show that deep learning applied to SDO data can be successfully used to create a high-fidelity virtual telescope that generates synthetic observations of the solar corona by image translation. Towards this end we developed a deep neural network, structured as an encoder-decoder with skip connections (U-Net), that reconstructs the Sun’s image of one instrument channel given temporally align... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Salvatelli2019Using.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Valentina Salvatelli, Souvik Bose, Brad Neuberg, Luiz F. G. dos Santos, Mark Cheung, Miho Janvier, Atilim Gunes Baydin, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Meng Jin
    <br>
    <i><b>Machine Learning and the Physical Sciences Workshop (ML4PS), NeurIPS 2019</b></i> <br> [<a href="https://arxiv.org/abs/1911.04006" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Salvatelli2019Using.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Neuberg2019Auto" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1911.04008" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astro.jpg" title="Auto-Calibration of Remote Sensing Solar Telescopes with Deep Learning" alt="Auto-Calibration of Remote Sensing Solar Telescopes with Deep Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Auto-Calibration of Remote Sensing Solar Telescopes with Deep Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Auto-Calibration of Remote Sensing Solar Telescopes with Deep Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>As a part of NASA’s Heliophysics System Observatory (HSO) fleet of satellites,the Solar Dynamics Observatory (SDO) has continuously monitored the Sun since2010. Ultraviolet (UV) and Extreme UV (EUV) instruments in orbit, such asSDO’s Atmospheric Imaging Assembly (AIA) instrument, suffer time-dependent degradation which reduces instrument sensitivity. Accurate calibration for (E)UV instruments currently depends on periodic sounding rockets, which are infrequent and not practical for heliophysics missions in deep space. In the present work, we develop a Convolutional Neural Network (CNN) that auto-calibrates SDO/AIA channels and corrects sensitivity degradation by exploiting spatial patterns in multi-wavelength observations to arrive at a self-calibration of (E)UV imaging instruments. Our results remove a major impediment to developing future HSOmissions of the same scientific caliber as SDO but in deep space, able to observe the Sun from more vantage points than just SDO’s curren... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Neuberg2019Auto.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Brad Neuberg, Souvik Bose, Valentina Salvatelli, Luiz F.G. dos Santos, Mark Cheung, Miho Janvier, Atilim Gunes Baydin, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Meng Jin
    <br>
    <i><b>Machine Learning and the Physical Sciences Workshop (ML4PS), NeurIPS 2019</b></i> <br> [<a href="https://arxiv.org/abs/1911.04008" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Neuberg2019Auto.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Lyle2019WiML" style="text-align: center;">
    <span class="images">
      <a href="https://docs.google.com/document/d/1drH0_3Gv_4LYQzImJBn6_0XjFE8tfHl94BD6fKyVbis/edit" target="_blank">
        <img src="images/neural_network.jpg" title="PAC-Bayes Generalization Bounds for Invariant Neural Networks" alt="PAC-Bayes Generalization Bounds for Invariant Neural Networks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>PAC-Bayes Generalization Bounds for Invariant Neural Networks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>PAC-Bayes Generalization Bounds for Invariant Neural Networks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Invariance is widely described as a desirable property of neural networks, but the mechanisms by which it benefits deep learning remain shrouded in mystery. We show that building invariance into model architecture via feature averaging provably tightens PAC-Bayes generalization bounds, as compared to data augmentation. Furthermore, through a link to the marginal likelihood and Bayesian model selection, we provide justification for using the improvement in these bounds for model selection. Our key observation is that invariance doesn’t just reduce variance in deep learning: it also changes the parameter-function mapping, and this leads better provable guarantees for the model.  We verify our theoretical results empirically on a permutation-invariant dataset.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, Marta Kwiatkowska, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>14th Women in Machine Learning Workshop (WiML 2019)</b></i> <br> [<a href="https://docs.google.com/document/d/1drH0_3Gv_4LYQzImJBn6_0XjFE8tfHl94BD6fKyVbis" target="_blank">WiML</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Lyle2019WiML.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Lamb2019Prediction" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1910.01570" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astro.jpg" title="Prediction of GNSS Phase Scintillations: A Machine Learning Approach" alt="Prediction of GNSS Phase Scintillations: A Machine Learning Approach" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Prediction of GNSS Phase Scintillations: A Machine Learning Approach</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Prediction of GNSS Phase Scintillations: A Machine Learning Approach</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>A Global Navigation Satellite System (GNSS) uses a constellation of satellites around the earth for accurate navigation, timing, and positioning. Natural phenomena like space weather introduce irregularities in the Earth’s ionosphere, disrupting the propagation of the radio signals that GNSS relies upon. Such disruptions affect both the amplitude and the phase of the propagated waves. No physics-based model currently exists to predict the time and location of these disruptions with sufficient accuracy and at relevant scales. In this paper, we focus on predicting the phase fluctuations of GNSS radio waves, known as phase scintillations. We propose a novel architecture and loss function to predict 1 hour in advance the magnitude of phase scintillations within a time window of plus-minus 5 minutes with state-of-the-art performance.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Kara Lamb, Garima Malhotra, Athanasios Vlontzos, Edward Wagstaff, Atılım Günes Baydin, Anahita Bhiwandiwalla, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Alfredo Kalaitzis, Anthony Reina, Asti Bhatt
    <br>
    <i><b>Machine Learning and the Physical Sciences Workshop (ML4PS), NeurIPS 2019</b></i> <br> [<a href="https://arxiv.org/abs/1910.01570" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Lamb2019Prediction.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Lamb2019Correlation" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1910.03085" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astro.jpg" title="Correlation of Auroral Dynamics and GNSS Scintillation with an Autoencoder" alt="Correlation of Auroral Dynamics and GNSS Scintillation with an Autoencoder" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Correlation of Auroral Dynamics and GNSS Scintillation with an Autoencoder</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Correlation of Auroral Dynamics and GNSS Scintillation with an Autoencoder</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>High energy particles originating from solar activity travel along the the Earth’s magnetic field and interact with the atmosphere around the higher latitudes. These interactions often manifest as aurora in the form of visible light in the Earth’s ionosphere. These interactions also result in irregularities in the electron density, which cause disruptions in the amplitude and phase of the radio signals from the Global Navigation Satellite Systems (GNSS), known as ‘scintillation’. In this paper we use a multi-scale residual autoencoder (Res-AE) to show the correlation between specific dynamic structures of the aurora and the magnitude of the GNSS phase scintillations (σϕ). Auroral images are encoded in a lower dimensional feature space using the Res-AE, which in turn are clustered with t-SNE and UMAP. Both methods produce similar clusters, and specific clusters demonstrate greater correlations with observed phase scintillations. Our results suggest that specific dynamic structure... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Lamb2019Correlation.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Kara Lamb, Garima Malhotra, Athanasios Vlontzos, Edward Wagstaff, Atılım Günes Baydin, Anahita Bhiwandiwalla, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Alfredo Kalaitzis, Anthony Reina, Asti Bhatt
    <br>
    <i><b>Machine Learning and the Physical Sciences Workshop (ML4PS), NeurIPS 2019</b></i> <br> [<a href="https://arxiv.org/abs/1910.03085" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Lamb2019Correlation.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Jungbluth2019Single" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1911.01490" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astro.jpg" title="Single-Frame Super-Resolution of Solar Magnetograms: Investigating Physics-Based Metrics & Losses" alt="Single-Frame Super-Resolution of Solar Magnetograms: Investigating Physics-Based Metrics & Losses" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Single-Frame Super-Resolution of Solar Magnetograms: Investigating Physics-Based Metrics & Losses</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Single-Frame Super-Resolution of Solar Magnetograms: Investigating Physics-Based Metrics & Losses</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Breakthroughs in our understanding of physical phenomena have traditionally followed improvements in instrumentation. Studies of the magnetic field of the Sun, and its influence on the solar dynamo and space weather events, have benefited from improvements in resolution and measurement frequency of new instruments. However, in order to fully understand the solar cycle, high-quality data across time-scales longer than the typical lifespan of a solar instrument are required. At the moment, discrepancies between measurement surveys prevent the combined use of all available data. In this work, we show that machine learning can help bridge the gap between measurement surveys by learning to super-resolve low-resolution magnetic field images and translate between characteristics of contemporary instruments in orbit. We also introduce the notion of physics-based metrics and losses for super-resolution to preserve underlying physics and constrain the solution space of possible super-reso... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Jungbluth2019Single.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Anna Jungbluth, Xavier Gitiaux, Shane A.Maloney, Carl Shneider, Paul J. Wright, Alfredo Kalaitzis, Michel Deudon, Atılım Güneş Baydin, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Andrés Muñoz-Jaramillo
    <br>
    <i><b>Machine Learning and the Physical Sciences Workshop (ML4PS), NeurIPS 2019</b></i> <br> [<a href="https://arxiv.org/abs/1911.01490" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Jungbluth2019Single.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Xiao2019Wat" style="text-align: center;">
    <span class="images">
      <a href="http://bayesiandeeplearning.org/2019/papers/90.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/uncertainty_small.jpg" title="Wat heb je gezegd? Detecting Out-of-Distribution Translations with Variational Transformers" alt="Wat heb je gezegd? Detecting Out-of-Distribution Translations with Variational Transformers" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Wat heb je gezegd? Detecting Out-of-Distribution Translations with Variational Transformers</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Wat heb je gezegd? Detecting Out-of-Distribution Translations with Variational Transformers</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We use epistemic uncertainty to detect out-of-training-distribution sentences in Neural Machine Translation. For this, we develop a measure of uncertainty designed specifically for long sequences of discrete random variables, corresponding to the words in the output sentence. This measure is able to convey epistemic uncertainty akin to the Mutual Information (MI), which is used in the case of single discrete random variables such as in classification. Our new measure of uncertainty solves a major intractability in the naive application of existing approaches on long sentences. We train a Transformer model with dropout on the task of GermanEnglish translation using WMT 13 and Europarl, and show that using dropout uncertainty our measure is able to identify when Dutch source sentences, sentences which use the same word types as German, are given to the model instead of German.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Tim Xiao, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><span style="color:red">Spotlight talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/90.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Xiao2019Wat.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Vicens2019Adversarial" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1912.04242" target="_blank">
        <img src="images/neural_network.jpg" title="Adversarial recovery of agent rewards from latent spaces of the limit order book" alt="Adversarial recovery of agent rewards from latent spaces of the limit order book" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Adversarial recovery of agent rewards from latent spaces of the limit order book</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Adversarial recovery of agent rewards from latent spaces of the limit order book</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Inverse reinforcement learning has proved its ability to explain state-action trajectories of expert agents by recovering their underlying reward functions in increasingly challenging environments. Recent advances in adversarial learning have allowed extending inverse RL to applications with non-stationary environment dynamics unknown to the agents, arbitrary structures of reward functions and improved handling of the ambiguities inherent to the ill-posed nature of inverse RL. This is particularly relevant in real time applications on stochastic environments involving risk, like volatile financial markets. Moreover, recent work on simulation of complex environments enable learning algorithms to engage with real market data through simulations of its latent space representations, avoiding a costly exploration of the original environment. In this paper, we explore whether adversarial inverse RL algorithms can be adapted and trained within such latent space simulations from real ma... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Vicens2019Adversarial.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Jacobo Roa Vicens, Yuanbo Wang, Virgile Mison, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Ricardo Silva
    <br>
    <b><i>NeurIPS 2019 Workshop on Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness, and Privacy</i></b> <br> [<a href="https://arxiv.org/abs/1912.04242" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Vicens2019Adversarial.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Veitch-Michaelis2019Flood" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1910.03019" target="_blank">
        <img src="images/esa_talk.jpg" title="Flood Detection On Low Cost Orbital Hardware" alt="Flood Detection On Low Cost Orbital Hardware" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Flood Detection On Low Cost Orbital Hardware</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Flood Detection On Low Cost Orbital Hardware</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Satellite imaging is a critical technology for monitoring and responding to natural disasters such as flooding. Despite the capabilities of modern satellites, there is still much to be desired from the perspective of first response organisations like UNICEF. Two main challenges are rapid access to data, and the ability to automatically identify flooded regions in images. We describe a prototypical flood segmentation system, identifying cloud, water and land, that could be deployed on a constellation of small satellites, performing processing on board to reduce downlink bandwidth by 2 orders of magnitude. We target PhiSat-1, part of the FSSCAT mission, which is planned to be launched by the European Space Agency (ESA) near the start of 2020 as a proof of concept for this new technology.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Joshua Veitch-Michaelis, Gonzalo Mateo-Garcia, Silviu Oprea, <a href="members/lewis_smith/index.html">Lewis Smith</a>, Atilim Gunes Baydin, Dietmar Backes, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Guy Schumann
    <br>
    <b><span style="color:red">Spotlight talk</span>, <i>Artificial Intelligence for Humanitarian Assistance and Disaster Response (AI+HADR) NeurIPS 2019 Workshop</i></b> <br> [<a href="https://arxiv.org/abs/1910.03019" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Veitch-Michaelis2019Flood.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Tigas2019Robust" style="text-align: center;">
    <span class="images">
      <a href="https://ml4ad.github.io/files/papers/Robust%20Imitative%20Planning:%20Planning%20from%20Demonstrations%20Under%20Uncertainty.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/cv_uncertainty.jpg" title="Robust Imitative Planning: Planning from Demonstrations Under Uncertainty" alt="Robust Imitative Planning: Planning from Demonstrations Under Uncertainty" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Robust Imitative Planning: Planning from Demonstrations Under Uncertainty</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Robust Imitative Planning: Planning from Demonstrations Under Uncertainty</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Learning from expert demonstrations is an attractive framework for sequential decision-making in safety-critical domains such as autonomous driving, where trial and error learning has no safety guarantees during training. However, naïve use of imitation learning can fail by extrapolating incorrectly to unfamiliar situations, resulting in arbitrary model outputs and dangerous outcomes. This is especially true for high capacity parametric models such as deep neural networks, for processing high-dimensional observations from cameras or LIDAR. Instead, we model expert behaviour with a model able to capture uncertainty about previously unseen scenarios, as well as inherent stochasticity in expert demonstrations. We propose a framework for planning under epistemic uncertainty and also provide a practical realisation, called robust imitative planning (RIP), using an ensemble of deep neural density estimators. We demonstrate online robustness to out-of-training distribution scenarios on... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Tigas2019Robust.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, <a href="members/angelos_filos/index.html">Angelos Filos</a>, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><i>NeurIPS2019 Workshop on Machine Learning for Autonomous Driving</i></b> <br> [<a href="https://ml4ad.github.io/files/papers/Robust%20Imitative%20Planning:%20Planning%20from%20Demonstrations%20Under%20Uncertainty.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Tigas2019Robust.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Simoes2019FDL" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1912.09279" target="_blank">
        <img src="images/esa_talk.jpg" title="FDL: Mission Support Challenge" alt="FDL: Mission Support Challenge" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>FDL: Mission Support Challenge</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>FDL: Mission Support Challenge</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>The Frontier Development Lab (FDL) is a National Aeronautics and Space Administration (NASA) machine learning program with the stated aim of conducting artificial intelligence research for space exploration and all humankind with support in the European program from the European Space Agency (ESA). Interdisciplinary teams of researchers and data-scientists are brought together to tackle a range of challenging, real-world problems in the space-domain. The program primarily consists of a sprint phase during which teams tackle separate problems in the spirit of ‘coopetition’. Teams are given a problem brief by real stakeholders and mentored by a range of experts. With access to exceptional computational resources, we were challenged to make a serious contribution within just eight weeks.
Stated simply, our team was tasked with producing a system capable of scheduling downloads from satellites autonomously. Scheduling is a difficult problem in general, of course, complicated further... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Simoes2019FDL.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Luís F. Simões, Ben Day, Vinutha M. Shreenath, Callum Wilson, Chris Bridges, Sylvester Kaczmarek, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><i>NeurIPS 2019 Workshop on Machine Learning Competitions for All</i></b> <br> [<a href="https://arxiv.org/abs/1912.09279" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Simoes2019FDL.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Sidrane2019Machine" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1910.06521" target="_blank">
        <img src="images/esa_talk.jpg" title="Machine Learning for Generalizable Prediction of Flood Susceptibility" alt="Machine Learning for Generalizable Prediction of Flood Susceptibility" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Machine Learning for Generalizable Prediction of Flood Susceptibility</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Machine Learning for Generalizable Prediction of Flood Susceptibility</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Flooding is a destructive and dangerous hazard and climate change appears to be increasing the frequency of catastrophic flooding events around the world. Physics-based flood models are costly to calibrate and are rarely generalizable across different river basins, as model outputs are sensitive to site-specific parameters and human-regulated infrastructure. In contrast, statistical models implicitly account for such factors through the data on which they are trained. Such models trained primarily from remotely-sensed Earth observation data could reduce the need for extensive in-situ measurements. In this work, we develop generalizable, multi-basin models of river flooding susceptibility using geographically-distributed data from the USGS stream gauge network. Machine learning models are trained in a supervised framework to predict two measures of flood susceptibility from a mix of river basin attributes, impervious surface cover information derived from satellite imagery, and h... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Sidrane2019Machine.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Chelsea Sidrane, Dylan J Fitzpatrick, Andrew Annex, Diane O’Donoghue, Piotr Bilinksi, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><span style="color:red">Spotlight talk</span>, <i>Artificial Intelligence for Humanitarian Assistance and Disaster Response (AI+HADR) NeurIPS 2019 Workshop</i></b> <br> [<a href="https://arxiv.org/abs/1910.06521" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Sidrane2019Machine.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Saxena2019Location" style="text-align: center;">
    <span class="images">
      <a href="https://neurips2019creativity.github.io/doc/Location%20Conditional%20Image%20Generation%20using%20Generative%20Adversarial%20Networks.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/art_oxford.png" title="Location Conditional Image Generation using Generative Adversarial Networks" alt="Location Conditional Image Generation using Generative Adversarial Networks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Location Conditional Image Generation using Generative Adversarial Networks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Location Conditional Image Generation using Generative Adversarial Networks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Can an AI-artist instil the emotion of sense of place in its audience? Motivated by this thought, this paper presents our endeavours to make a GANs model learn the visual characteristics of locations to achieve creativity. The project’s novelty lies in addressing the problem of the hardness of GANs training for an extremely diverse dataset in a contextual setting. The project explores GANs as an impressionist artist who adds its perspective to the artwork without hampering photo realism.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Mayur Saxena, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><i>Machine Learning for Creativity and Design NeurIPS 2019 Workshop</i></b> <br> [<a href="https://neurips2019creativity.github.io/doc/Location%20Conditional%20Image%20Generation%20using%20Generative%20Adversarial%20Networks.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Saxena2019Location.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Rudner2019Natural" style="text-align: center;">
    <span class="images">
      <a href="http://timrudner.com/assets/papers/Natural_Neural_Tangent_Kernel.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/uncertainty_small.jpg" title="The Natural Neural Tangent Kernel: Neural Network Training Dynamics under Natural Gradient Descent" alt="The Natural Neural Tangent Kernel: Neural Network Training Dynamics under Natural Gradient Descent" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>The Natural Neural Tangent Kernel: Neural Network Training Dynamics under Natural Gradient Descent</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>The Natural Neural Tangent Kernel: Neural Network Training Dynamics under Natural Gradient Descent</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Gradient-based optimization methods have proven successful in learning complex,
overparameterized neural networks from non-convex objectives. Yet, the precise
theoretical relationship between gradient-based optimization methods, the induced
training dynamics, and generalization in deep neural networks remains unclear.
In this work, we investigate the training dynamics of overparameterized neural
networks under natural gradient descent. Taking a function-space view of the
training dynamics, we give an exact analytic solution to the training dynamics on
training points. We derive a bound on the discrepancy between the distributions over
functions at the global optimum of natural gradient descent and the analytic solution
to the natural gradient descent training dynamics linearized around the parameters
at initialization and validate our theoretical results empirically. In particular, we
show that the discrepancy between the functions obtained from linearized and
non-linearized nat... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Rudner2019Natural.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, Florian Wenzel, Yee Whye Teh, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><span style="color:red">Contributed talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://timrudner.com/assets/papers/Natural_Neural_Tangent_Kernel.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Rudner2019Natural.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Krishnan2019Improving" style="text-align: center;">
    <span class="images">
      <a href="http://bayesiandeeplearning.org/2019/papers/94.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/uncertainty_small.jpg" title="Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis" alt="Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Specifying meaningful weight priors for variational inference in Bayesian deep
neural network (DNN) is a challenging problem, particularly for scaling to larger
models involving high dimensional weight space. We evaluate the recently proposed, MOdel Priors with Empirical Bayes using DNN (MOPED) method for
Bayesian DNNs within the Bayesian Deep Learning (BDL) benchmarking framework. MOPED enables scalable VI in large models by providing a way to choose
informed prior and approximate posterior distributions for Bayesian neural network
weights using Empirical Bayes framework. We benchmark MOPED with mean
field variational inference on a real-world diabetic retinopathy diagnosis task and
compare with state-of-the-art BDL techniques. We demonstrate MOPED method
provides reliable uncertainty estimates while outperforming state-of-the-art methods, offering a new strong baseline for the BDL community to compare on complex
real-world tasks involving larger models.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, <a href="members/angelos_filos/index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <b><i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/94.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Krishnan2019Improving.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Gitiaux2019Probabilistic" style="text-align: center;">
    <span class="images">
      <a href="http://bayesiandeeplearning.org/2019/papers/120.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/uncertainty_small.jpg" title="Probabilistic Super-Resolution of Solar Magnetograms: Generating Many Explanations and Measuring Uncertainties" alt="Probabilistic Super-Resolution of Solar Magnetograms: Generating Many Explanations and Measuring Uncertainties" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Probabilistic Super-Resolution of Solar Magnetograms: Generating Many Explanations and Measuring Uncertainties</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Probabilistic Super-Resolution of Solar Magnetograms: Generating Many Explanations and Measuring Uncertainties</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Machine learning techniques have been successfully applied to super-resolution
tasks on natural images where visually pleasing results are sufficient. However
in many scientific domains this is not adequate and estimations of errors and
uncertainties are crucial. To address this issue we propose a Bayesian framework
that decomposes uncertainties into epistemic and aleatoric uncertainties. We test the
validity of our approach by super-resolving images of the Sun’s magnetic field and
by generating maps measuring the range of possible high resolution explanations
compatible with a given low resolution magnetogram.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Xavier Gitiaux, Shane Maloney, Anna Jungbluth, Carl Shneider, Atılım Güneş Baydin, Paul J. Wright, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Michel Deudon, Alfredo Kalaitzis, Andres Munoz-Jaramillo
    <br>
    <b><i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/120.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Gitiaux2019Probabilistic.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="OATML2019DiabeticRetinopathyDiagnosis" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1912.10481" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/diabetic_retinopathy_diagnosis.jpg" title="A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks" alt="A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods’ robustness and scalability, assessing whether new tools give ‘better’ uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on diabetic retinopathy diagnosis. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening—i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert... [<a href="https://oatml.cs.ox.ac.uk/publications/201907_OATML2019DiabeticRetinopathyDiagnosis.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/angelos_filos/index.html">Angelos Filos</a>, <a href="members/sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="members/zac_kenton/index.html">Zac Kenton</a>, <a href="members/lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, Arnoud de Kroon, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i>Preprint, 2019</i> <br> [<a href="http://www.cs.ox.ac.uk/people/angelos.filos/publications/diabetic_retinopathy_diagnosis.pdf" target="_blank">Preprint</a>] [<a href="bibtex/OATML2019DiabeticRetinopathyDiagnosis.bib" target="_blank">BibTex</a>] [<a href="https://github.com/OATML/bdl-benchmarks" target="_blank">Code</a>] <br> <i>arXiv, 2019</i> <br> [<a href="http://arxiv.org/abs/1912.10481" target="_blank">arXiv</a>] <br> <b><span style="color:red">Spotlight talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/12.pdf" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201907_OATML2019DiabeticRetinopathyDiagnosis.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="KAJvAYGBatchBALD" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1906.08158" target="_blank">
        <img src="images/batchbald_teaser.png" title="BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning" alt="BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time 1−1/e-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/andreas_kirsch/index.html">Andreas Kirsch</a>, <a href="members/joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>NeurIPS, 2019</b></i> <br> [<a href="https://arxiv.org/abs/1906.08158" target="_blank">arXiv</a>] [<a href="bibtex/KAJvAYGBatchBALD.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_KAJvAYGBatchBALD.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Fellows2019VIREL" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1811.01132" target="_blank">
        <img src="images/virel.jpg" title="VIREL: A Variational Inference Framework for Reinforcement Learning" alt="VIREL: A Variational Inference Framework for Reinforcement Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>VIREL: A Variational Inference Framework for Reinforcement Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>VIREL: A Variational Inference Framework for Reinforcement Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Applying probabilistic models to reinforcement learning (RL) enables the application of powerful optimisation tools such as variational inference to RL. However, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, e.g., the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties learning deterministic policies in maximum entropy RL based approaches. We propose VIREL, a novel, theoretically grounded probabilistic inference framework for RL that utilises a parametrised action-value function to summarise future dynamics of the underlying MDP. This gives VIREL a mode-seeking form of KL divergence, the ability to learn deterministic optimal polices naturally from inference and the ability to optimise value functions and policies in separate, iterative steps. In applying variational expectation-maximisation to VIREL we thus show that the actor-critic algorithm can be reduced to expectation-maximisation... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Fellows2019Virel.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Matthew Fellows, Anuj Mahajan, <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, Shimon Whiteson
    <br>
    <i><b>NeurIPS, 2019</b></i> <br> <i>NeurIPS 2018 Workshop on Probabilistic Reinforcement Learning and Structured Control</i> <br> [<a href="https://arxiv.org/abs/1811.01132" target="_blank">arXiv</a>] [<a href="bibtex/Fellows2019Virel.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Fellows2019Virel.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Bellemare2019Geometric" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1901.11530" target="_blank">
        <img src="images/neural_network.jpg" title="A Geometric Perspective on Optimal Representations for Reinforcement Learning" alt="A Geometric Perspective on Optimal Representations for Reinforcement Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>A Geometric Perspective on Optimal Representations for Reinforcement Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>A Geometric Perspective on Optimal Representations for Reinforcement Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>
    <br>
    <i><b>NeurIPS, 2019</b></i> <br> [<a href="https://arxiv.org/abs/1901.11530" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Bellemare2019Geometric.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Locatello2019Benefits" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1905.13662" target="_blank">
        <img src="images/neural_network.jpg" title="On the Benefits of Disentangled Representations" alt="On the Benefits of Disentangled Representations" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>On the Benefits of Disentangled Representations</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>On the Benefits of Disentangled Representations</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing more than 12600 trained representations of state-of-the-art disentangled models, we observe that various disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglemen... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Locatello2019Benefits.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Francesco Locatello, Gabriele Abbati, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>, Stefan Bauer, Bernhard Schölkopf, Olivier Bachem
    <br>
    <i><b>NeurIPS, 2019</b></i> <br> [<a href="https://arxiv.org/abs/1905.13662" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Locatello2019Benefits.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Foster2019Variational" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1903.05480" target="_blank">
        <img src="images/neural_network.jpg" title="Variational Bayesian Optimal Experimental Design" alt="Variational Bayesian Optimal Experimental Design" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Variational Bayesian Optimal Experimental Design</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Variational Bayesian Optimal Experimental Design</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee Whye Teh, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>, Noah Goodman
    <br>
    <i><b>NeurIPS, 2019</b></i> <br> [<a href="https://arxiv.org/abs/1903.05480" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201912_Foster2019Variational.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Lyle2019Understanding" style="text-align: center;">
    <span class="images">
      <a href="https://drive.google.com/file/d/1dPai93Z3UPsZh6SWojBdMZnEMlxiaV2M/view" target="_blank">
        <img src="images/neural_network.jpg" title="An Analysis of the Effect of Invariance on Generalization in Neural Networks" alt="An Analysis of the Effect of Invariance on Generalization in Neural Networks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>An Analysis of the Effect of Invariance on Generalization in Neural Networks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>An Analysis of the Effect of Invariance on Generalization in Neural Networks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Invariance is often cited as a desirable property of machine learning systems, claimed to improve model accuracy and reduce overfitting. Empirically, invariant models often generalize better than their non-invariant counterparts. But is it possible to show that invariant models provably do so? In this paper we explore the effect of invariance on model generalization. We find strong Bayesian and frequentist motivations for enforcing invariance which leverage recent results connecting PAC-Bayes generalization bounds and the marginal likelihood. We make use of these results to perform model selection on neural networks.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, Marta Kwiatkowska, Mark van der Wilk, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i>Understanding and Improving Generalization in Deep Learning workshop, ICML, 2019</i> <br> [<a href="https://drive.google.com/file/d/1dPai93Z3UPsZh6SWojBdMZnEMlxiaV2M/view" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201906_Lyle2019Understanding.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Walmsley2019Galaxy" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1905.07424" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/galaxy_zoo.png" title="Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning" alt="Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We use Bayesian CNNs and a novel generative model of Galaxy Zoo volunteer responses to infer posteriors for the visual morphology of galaxies. Bayesian CNN can learn from galaxy images with uncertain labels and then, for previously unlabelled galaxies, predict the probability of each possible label. Using our posteriors, we apply the active learning strategy BALD to request volunteer responses for the subset of galaxies which, if labelled, would be most informative for training our network. By combining human and machine intelligence, Galaxy Zoo will be able to classify surveys of any conceivable scale on a timescale of weeks, providing massive and detailed morphology catalogues to support research into galaxy evolution.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Mike Walmsley, <a href="members/lewis_smith/index.html">Lewis Smith</a>, Chris Lintott, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Steven Bamford, Hugh Dickinson, Lucy Fortson, Sandor Kruk, Karen Masters, Claudia Scarlata, Brooke Simmons, Rebecca Smethurst, Darryl Wright
    <br>
    <b><i>Monthly Notices of the Royal Astronomical Society, 2019</i></b><br> [<a href="https://academic.oup.com/mnras/advance-article-abstract/doi/10.1093/mnras/stz2816/5583078?redirectedFrom=fulltext" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1905.07424" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201905_Walmsley2019Galaxy.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Cobb2019Ensemble" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1905.10659" target="_blank">
        <img src="images/inara-results-12mol.png" title="An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval" alt="An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>An Ensemble of Bayesian Neural Networks for Exoplanetary Atmospheric Retrieval</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Recent work demonstrated the potential of using machine learning algorithms for atmospheric retrieval by implementing a random forest to perform retrievals in seconds that are consistent with the traditional, computationally-expensive nested-sampling retrieval method. We expand upon their approach by presenting a new machine learning model, 	exttt{plan-net}, based on an ensemble of Bayesian neural networks that yields more accurate inferences than the random forest for the same data set of synthetic transmission spectra.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Adam D. Cobb, Michael D. Himes, Frank Soboczenski, Simone Zorzan, Molly D. O'Beirne, Atılım Güneş Baydin, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Shawn D. Domagal-Goldman, Giada N. Arney, Daniel Angerhausen
    <br>
    <i><b>The Astronomical Journal, 2019</b></i> <br> [<a href="https://iopscience.iop.org/article/10.3847/1538-3881/ab2390/meta" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1905.10659" target="_blank">arXiv</a>] [<a href="https://github.com/exoml/plan-net" target="_blank">Code</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201905_Cobb2019Ensemble.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="RoaVicensChtourouFilosRulIanGalSilva2019MALICML" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1906.04813" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/RoaVicensChtourouFilosRulIanGalSilva2019MALICML.jpg" title="Towards Inverse Reinforcement Learning for Limit Order Book Dynamics" alt="Towards Inverse Reinforcement Learning for Limit Order Book Dynamics" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Towards Inverse Reinforcement Learning for Limit Order Book Dynamics</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Towards Inverse Reinforcement Learning for Limit Order Book Dynamics</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We investigate whether Inverse Reinforcement Learning (IRL) can infer rewards from agents within real financial stochastic environments: limit order books (LOB). Our results illustrate that complex behaviours, induced by non-linear reward functions amid agent-based stochastic scenarios, can be deduced through inference, encouraging the use of inverse reinforcement learning for opponent-modelling in multi-agent systems.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Jacobo Roa-Vicens, Cyrine Chtourou, <a href="members/angelos_filos/index.html">Angelos Filos</a>, Francisco Rullan, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Ricardo Silva
    <br>
    <i><b>Oral Presentation, Multi-Agent Learning Workshop at the 36th International Conference on Machine Learning, 2019</b></i> <br> [<a href="https://arxiv.org/abs/1906.04813" target="_blank">arXiv</a>] [<a href="bibtex/RoaVicensChtourouFilosRulIanGalSilva2019MALICML.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201904_RoaVicensChtourouFilosRulIanGalSilva2019MALICML.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Kenton2019Generalization" style="text-align: center;">
    <span class="images">
      <a href="https://zackenton.github.io/files/Generalizing_Limited_Environments.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/gridworld.jpg" title="Generalizing from a few environments in safety-critical reinforcement learning" alt="Generalizing from a few environments in safety-critical reinforcement learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Generalizing from a few environments in safety-critical reinforcement learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Generalizing from a few environments in safety-critical reinforcement learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training (e.g. many simulated environments), allowing them to learn about every possible danger. But this is often impractical: simulations may fail to capture the full range of situations and may differ subtly from reality. This paper investigates generalizing from a limited number of training environments in deep reinforcement learning. Our experiments test whether agents can perform safely in novel environments, given varying numbers of environments at train time. Using a gridworld setting, we find that standard deep RL agents do not reliably avoid catastrophes on unseen environments – even after performing near optimally on 1000 training environments. However, we show that catastrophes can be significantly reduced (but not eliminated) with simple modifications, including Q-network... [<a href="https://oatml.cs.ox.ac.uk/publications/201903_Kenton2019Generalization.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/zac_kenton/index.html">Zac Kenton</a>, <a href="members/angelos_filos/index.html">Angelos Filos</a>, Owain Evans, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>ICLR 2019 Workshop on Safe Machine Learning</b></i> <br> [<a href="https://zackenton.github.io/files/Generalizing_Limited_Environments.pdf" target="_blank">paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201903_Kenton2019Generalization.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Samvelyan2019SMAC" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1902.04043" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/smac.jpg" title="The StarCraft Multi-Agent Challenge" alt="The StarCraft Multi-Agent Challenge" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>The StarCraft Multi-Agent Challenge</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>The StarCraft Multi-Agent Challenge</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time... [<a href="https://oatml.cs.ox.ac.uk/publications/201902_Samvelyan2019SMAC.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, Chia-Man Hung, Philip H. S. Torr, Jakob Foerster, Shimon Whiteson
    <br>
    <i><b>AAMAS 2019</b></i> <br> <i>NeurIPS 2019 Workshop on Deep Reinforcement Learning</i> <br> [<a href="https://arxiv.org/abs/1902.04043" target="_blank">arXiv</a>] [<a href="https://github.com/oxwhirl/smac" target="_blank">Code</a>] [<a href="bibtex/Samvelyan2019SMAC.bib" target="_blank">BibTex</a>] [<a href="http://whirl.cs.ox.ac.uk/blog/smac" target="_blank">Media</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201902_Samvelyan2019SMAC.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Rudner2019Multi3Net" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1812.01756" target="_blank">
        <img src="images/multi3net.jpg" title="Multi&sup3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery" alt="Multi&sup3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Multi&sup3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Multi&sup3Net: Segmenting Flooded Buildings via Fusion of Multiresolution, Multisensor, and Multitemporal Satellite Imagery</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We propose a novel approach for rapid segmentation of flooded buildings by fusing multiresolution, multisensor, and multitemporal satellite imagery in a convolutional neural network. Our model significantly expedites the generation of satellite imagery-based flood maps, crucial for first responders and local authorities in the early stages of flood events. By incorporating multitemporal satellite imagery, our model allows for rapid and accurate post-disaster damage assessment and can be used by governments to better coordinate medium- and long-term financial assistance programs for affected areas. The network consists of multiple streams of encoder-decoder architectures that extract spatiotemporal information from medium-resolution images and spatial information from high-resolution images before fusing the resulting representations into a single medium-resolution segmentation map of flooded buildings. We compare our model to state-of-the-art methods for building footprint segme... [<a href="https://oatml.cs.ox.ac.uk/publications/201902_Rudner2019Multi3Net.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, Marc Rußwurm, Jakub Fil, Ramona Pelich, Benjamin Bischke, Veronika Kopackova, Piotr Bilinski
    <br>
    <i><b>AAAI 2019</b></i> <br> <i>NeurIPS 2018 Workshop AI for Social Good</i> <br> [<a href="https://arxiv.org/abs/1812.01756" target="_blank">arXiv</a>] [<a href="https://github.com/FrontierDevelopmentLab/multi3net" target="_blank">Code</a>] [<a href="bibtex/Rudner2019Multi3Net.bib" target="_blank">BibTex</a>] [<a href="https://fdleurope.org/fdl-europe-2018#block-yui_3_17_2_1_1549364366960_29703" target="_blank">Media</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201902_Rudner2019Multi3Net.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Lyle2019Comparative" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1901.11084" target="_blank">
        <img src="images/neural_network.jpg" title="A Comparative Analysis of Distributional and Expected Reinforcement Learning" alt="A Comparative Analysis of Distributional and Expected Reinforcement Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>A Comparative Analysis of Distributional and Expected Reinforcement Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>A Comparative Analysis of Distributional and Expected Reinforcement Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Since their introduction a year ago, distributional approaches to reinforcement learning (distributional RL) have produced strong results relative to the standard approach which models expected values (expected RL). However, aside from convergence guarantees, there have been few theoretical results investigating the reasons behind the improvements distributional RL provides. In this paper we begin the investigation into this fundamental question by analyzing the differences in the tabular, linear approximation, and non-linear approximation settings. We prove that in many realizations of the tabular and linear approximation settings, distributional RL behaves exactly the same as expected RL. In cases where the two methods behave differently, distributional RL can in fact hurt performance when it does not induce identical behaviour. We then continue with an empirical analysis comparing distributional and expected RL methods in control settings with non-linear approximators to teas... [<a href="https://oatml.cs.ox.ac.uk/publications/201902_Lyle2019Comparative.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, Pablo Samuel Castro, Marc G Bellemare
    <br>
    <i><b>AAAI 2019</b></i><br>[<a href="https://arxiv.org/abs/1901.11084" target="_blank">Paper</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201902_Lyle2019Comparative.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Soboczenski2018Bayesian" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1811.03390v2" target="_blank">
        <img src="images/inara-results-12mol.png" title="Bayesian Deep Learning for Exoplanet Atmospheric Retrieval" alt="Bayesian Deep Learning for Exoplanet Atmospheric Retrieval" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Bayesian Deep Learning for Exoplanet Atmospheric Retrieval</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Bayesian Deep Learning for Exoplanet Atmospheric Retrieval</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>An ML-based retrieval framework called Intelligent exoplaNet Atmospheric RetrievAl (INARA) that consists of a Bayesian deep learning model for retrieval and a data set of 3,000,000 synthetic rocky exoplanetary spectra generated using the NASA Planetary Spectrum Generator.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Frank Soboczenski, Michael D. Himes, Molly D. O'Beirne, Simone Zorzan, Atilim Gunes Baydin, Adam D. Cobb, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Daniel Angerhausen, Massimo Mascaro, Giada N. Arney, Shawn D. Domagal-Goldman
    <br>
    <b><i>Workshop on Bayesian Deep Learning, NeurIPS 2018</i></b> <br> [<a href="https://arxiv.org/abs/1811.03390v2" target="_blank">arXiv</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_Soboczenski2018Bayesian.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="RudnerGalTeh2018NPsasGPs" style="text-align: center;">
    <span class="images">
      <a href="http://bayesiandeeplearning.org/2018/papers/128.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/nps_as_gps.jpg" title="On the Connection between Neural Processes and Gaussian Processes with Deep Kernels" alt="On the Connection between Neural Processes and Gaussian Processes with Deep Kernels" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>On the Connection between Neural Processes and Gaussian Processes with Deep Kernels</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>On the Connection between Neural Processes and Gaussian Processes with Deep Kernels</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Neural Processes (NPs) are a class of neural latent variable models that combine desirable properties of Gaussian Processes (GPs) and neural networks. Like GPs, NPs define distributions over functions and are able to estimate the uncertainty in their predictions. Like neural networks, NPs are computationally efficient during training and prediction time. We establish a simple and explicit connection between NPs and GPs. In particular, we show that, under certain conditions, NPs are mathematically equivalent to GPs with deep kernels. This result further elucidates the relationship between GPs and NPs and makes previously derived theoretical insights about GPs applicable to NPs. Furthermore, it suggests a novel approach to learning expressive GP covariance functions applicable across different prediction tasks by training a deep kernel GP on a set of datasets</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/tim_rudner/index.html">Tim G. J. Rudner</a>, Vincent Fortuin, Yee Whye Teh, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>Workshop on Bayesian Deep Learning, NeurIPS 2018</b></i> <br> [<a href="http://bayesiandeeplearning.org/2018/papers/128.pdf" target="_blank">Paper</a>] [<a href="bibtex/RudnerGalTeh2018NPsasGPs.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_RudnerGalTeh2018NPsasGPs.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="MukhotiGal2018Importance" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1811.09385" target="_blank">
        <img src="images/neural_network.jpg" title="On the Importance of Strong Baselines in Bayesian Deep Learning" alt="On the Importance of Strong Baselines in Bayesian Deep Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>On the Importance of Strong Baselines in Bayesian Deep Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>On the Importance of Strong Baselines in Bayesian Deep Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Like all sub-fields of machine learning, Bayesian Deep Learning is driven by empirical validation of its theoretical proposals. Given the many aspects of an experiment, it is always possible that minor or even major experimental flaws can slip by both authors and reviewers. One of the most popular experiments used to evaluate approximate inference techniques is the regression experiment on UCI datasets. However, in this experiment, models which have been trained to convergence have often been compared with baselines trained only for a fixed number of iterations. What we find is that if we take a well-established baseline and evaluate it under the same experimental settings, it shows significant improvements in performance. In fact, it outperforms or performs competitively with numerous to several methods that when they were introduced claimed to be superior to the very same baseline method. Hence, by exposing this flaw in experimental procedure, we highlight the importance of us... [<a href="https://oatml.cs.ox.ac.uk/publications/201812_MukhotiGal2018Importance.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Jishnu Mukhoti, Pontus Stenetorp, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>Workshop on Bayesian Deep Learning, NeurIPS 2018</b></i> <br> [<a href="http://bayesiandeeplearning.org/2018/papers/42.pdf" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1811.09385" target="_blank">arXiv</a>] [<a href="http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2018arXiv181109385M&data_type=BIBTEX&db_key=PRE&nocookieset=1" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_MukhotiGal2018Importance.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="MukhotiGal2018Evaluating" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1811.12709" target="_blank">
        <img src="images/neural_network.jpg" title="Evaluating Bayesian Deep Learning Methods for Semantic Segmentation" alt="Evaluating Bayesian Deep Learning Methods for Semantic Segmentation" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Evaluating Bayesian Deep Learning Methods for Semantic Segmentation</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Evaluating Bayesian Deep Learning Methods for Semantic Segmentation</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Deep learning has been revolutionary for computer vision and semantic segmentation in particular, with Bayesian Deep Learning (BDL) used to obtain uncertainty maps from deep models when predicting semantic classes. This information is critical when using semantic segmentation for autonomous driving for example. Standard semantic segmentation systems have well-established evaluation metrics. However, with BDL’s rising popularity in computer vision we require new metrics to evaluate whether a BDL method produces better uncertainty estimates than another method. In this work we propose three such metrics to evaluate BDL models designed specifically for the task of semantic segmentation. We modify DeepLab-v3+, one of the state-of-the-art deep neural networks, and create its Bayesian counterpart using MC dropout and Concrete dropout as inference techniques. We then compare and test these two inference techniques on the well-known Cityscapes dataset using our suggested metrics. Our re... [<a href="https://oatml.cs.ox.ac.uk/publications/201812_MukhotiGal2018Evaluating.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Jishnu Mukhoti, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i>arXiv</i> <br> [<a href="https://arxiv.org/abs/1811.12709" target="_blank">arXiv</a>] [<a href="http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2018arXiv181112709M&data_type=BIBTEX&db_key=PRE&nocookieset=1" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_MukhotiGal2018Evaluating.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="MichelmoreGal2018Eval" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1811.06817" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/cv_uncertainty.jpg" title="Evaluating Uncertainty Quantification in End-to-End Autonomous Driving Control" alt="Evaluating Uncertainty Quantification in End-to-End Autonomous Driving Control" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Evaluating Uncertainty Quantification in End-to-End Autonomous Driving Control</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Evaluating Uncertainty Quantification in End-to-End Autonomous Driving Control</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Self-driving has benefited from significant performance improvements with the rise of deep learning, with millions of miles having been driven with no human intervention. Despite this, crashes and erroneous behaviours still occur, in part due to the complexity of verifying the correctness of DNNs and a lack of safety guarantees. In this paper, we demonstrate how quantitative measures of uncertainty can be extracted in real-time, and their quality evaluated in end-to-end controllers for self-driving cars. We propose evaluation techniques for the uncertainty on two separate architectures which use the uncertainty to predict crashes up to five seconds in advance. We find that mutual information, a measure of uncertainty in classification networks, is a promising indicator of forthcoming crashes.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Rhiannon Michelmore, Marta Kwiatkowska, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i>In submission</i> <br> [<a href="https://arxiv.org/abs/1811.06817" target="_blank">arXiv</a>] [<a href="http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2018arXiv181106817M&data_type=BIBTEX&db_key=PRE&nocookieset=1" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_MichelmoreGal2018Eval.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="GomezGalHinton2018Targeted" style="text-align: center;">
    <span class="images">
      <a href="https://openreview.net/pdf?id=HkghWScuoQ" target="_blank">
        <img src="images/neural_network.jpg" title="Targeted Dropout" alt="Targeted Dropout" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Targeted Dropout</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Targeted Dropout</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Neural networks are extremely flexible models due to their large number of parameters, which is beneficial for learning, but also highly redundant. This makes it possible to compress neural networks without having a drastic effect on performance. We introduce targeted dropout, a strategy for post hoc pruning of neural network weights and units that builds the pruning mechanism directly into learning. At each weight update, targeted dropout selects a candidate set for pruning using a simple selection criterion, and then stochastically prunes the network via dropout applied to this set. The resulting network learns to be explicitly robust to pruning, comparing favourably to more complicated regularization schemes while at the same time being extremely simple to implement, and easy to tune.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="https://ivanzhang.ca/" target="_blank">Ivan Zhang</a>, <a href="http://www.cs.toronto.edu/~kswersky/" target="_blank">Kevin Swersky</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <a href="http://www.cs.toronto.edu/~hinton/" target="_blank">Geoffrey E. Hinton</a>
    <br>
    <i><b>Workshop on Compact Deep Neural Networks with industrial applications, NeurIPS 2018</b></i> <br> [<a href="https://openreview.net/pdf?id=HkghWScuoQ" target="_blank">Paper</a>] [BibTex]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_GomezGalHinton2018Targeted.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="FarquharGal2018Unifying" style="text-align: center;">
    <span class="images">
      <a href="http://sebastianfarquhar.com/assets/pdf/Farquhar2018c.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/continual_learning_image.jpg" title="A Unifying Bayesian View of Continual Learning" alt="A Unifying Bayesian View of Continual Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>A Unifying Bayesian View of Continual Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>A Unifying Bayesian View of Continual Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Some machine learning applications require continual learning—where data comes in a sequence of datasets, each is used for training and then permanently discarded. From a Bayesian perspective, continual learning seems straightforward: Given the model posterior one would simply use this as the prior for the next task. However, exact posterior evaluation is intractable with many models, especially with Bayesian neural networks (BNNs). Instead, posterior approximations are often sought. Unfortunately, when posterior approximations are used, prior-focused approaches do not succeed in evaluations designed to capture properties of realistic continual learning use cases. As an alternative to prior-focused methods, we introduce a new approximate Bayesian derivation of the continual learning loss. Our loss does not rely on the posterior from earlier tasks, and instead adapts the model itself by changing the likelihood term. We call these approaches likelihood-focused. We then combine pri... [<a href="https://oatml.cs.ox.ac.uk/publications/201812_FarquharGal2018Unifying.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>NeurIPS 2018 workshop on Bayesian Deep Learning</b></i> <br> [<a href="https://sebastianfarquhar.com/assets/pdf/Farquhar2018c.pdf" target="_blank">Paper</a>] [BibTex]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201812_FarquharGal2018Unifying.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="MarshallGal2018BayesianOpt" style="text-align: center;">
    <span class="images">
      <a href="http://adsabs.harvard.edu/abs/2018DPS....5050501M" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astroid.jpg" title="Using Bayesian Optimization to Find Asteroids' Pole Directions" alt="Using Bayesian Optimization to Find Asteroids' Pole Directions" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Using Bayesian Optimization to Find Asteroids' Pole Directions</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Using Bayesian Optimization to Find Asteroids' Pole Directions</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Near-Earth asteroids (NEAs) are being discovered much faster than their shapes and other physical properties can be characterized in detail. One of the best ways to spatially resolve NEAs from the ground is with planetary radar observations. Radar echoes can be decoded in round-trip travel time and frequency to produce two-dimensional delay-Doppler images of the asteroid. Given a series of such images acquired over the course of the asteroid’s rotation, one can search for the shape and other physical properties that best match the observations. However, reconstructing asteroid shapes from radar data is, like many inverse problems, a computationally intensive task. Shape modeling also requires extensive human oversight to ensure that the fitting process is finding physically reasonable results. In this paper we use Bayesian optimisation for this difficult task.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Marshall, Sean, Cobb, Adam, Raïssi, Chedy, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Rozek, Agata, Busch, Michael W., Young, Grace, McGlasson, Riley
    <br>
    <i><b>American Astronomical Society (AAS), 2018</b></i> <br> [<a href="http://adsabs.harvard.edu/abs/2018DPS....5050501M" target="_blank">Citation</a>] [<a href="http://adsabs.harvard.edu/cgi-bin/nph-bib_query?bibcode=2018DPS....5050501M&data_type=BIBTEX&db_key=AST&nocookieset=1" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201810_MarshallGal2018BayesianOpt.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="Alizadeh2019Empirical" style="text-align: center;">
    <span class="images">
      <a href="https://openreview.net/forum?id=rJfUCoR5KX" target="_blank">
        <img src="images/neural_network.jpg" title="An Empirical study of Binary Neural Networks' Optimisation" alt="An Empirical study of Binary Neural Networks' Optimisation" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>An Empirical study of Binary Neural Networks' Optimisation</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>An Empirical study of Binary Neural Networks' Optimisation</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Binary neural networks using the Straight-Through-Estimator (STE) have been shown to achieve state-of-the-art results, but their training process is not well-founded. This is due to the discrepancy between the evaluated function in the forward path, and the weight updates in the back-propagation, updates which do not correspond to gradients of the forward path. Efficient convergence and accuracy of binary models often rely on careful fine-tuning and various ad-hoc techniques. In this work, we empirically identify and study the effectiveness of the various ad-hoc techniques commonly used in the literature, providing best-practices for efficient training of binary models. We show that adapting learning rates using second moment methods is crucial for the successful use of the STE, and that other optimisers can easily get stuck in local minima. We also find that many of the commonly employed tricks are only effective towards the end of the training, with these methods making early ... [<a href="https://oatml.cs.ox.ac.uk/publications/201809_Alizadeh2018Empirical.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, Javier Fernández-Marqués, Nicholas D. Lane, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>International Conference on Learning Representations (ICLR), 2019</b></i><br>[<a href="https://openreview.net/forum?id=rJfUCoR5KX" target="_blank">Paper</a>] [<a href="https://github.com/mi-lad/studying-binary-neural-networks" target="_blank">Code</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201809_Alizadeh2018Empirical.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="KorshunovaGal2018BRUNO" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1802.07535" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/bruno.jpg" title="BRUNO: A Deep Recurrent Model for Exchangeable Data" alt="BRUNO: A Deep Recurrent Model for Exchangeable Data" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>BRUNO: A Deep Recurrent Model for Exchangeable Data</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>BRUNO: A Deep Recurrent Model for Exchangeable Data</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Iryna Korshunova, Jonas Degrave, Ferenc Huszár, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Arthur Gretton, Joni Dambre
    <br>
    <i>arXiv, 2018</i> <br> [<a href="https://arxiv.org/abs/1802.07535" target="_blank">arXiv</a>] [<a href="bibtex/KorshunovaGal2018BRUNO.bib" target="_blank">BibTex</a>] <br> <i><b>NIPS, 2018</b></i> <br> [<a href="https://papers.nips.cc/paper/7949-bruno-a-deep-recurrent-model-for-exchangeable-data.pdf" target="_blank">Paper</a>] [<a href="https://papers.nips.cc/paper/7949-bruno-a-deep-recurrent-model-for-exchangeable-data/bibtex" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201808_KorshunovaGal2018BRUNO.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="GalSmith2018Sufficient" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1806.00667" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/mmnist.jpg" title="Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks" alt="Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification... [<a href="https://oatml.cs.ox.ac.uk/publications/201808_GalSmith2018Sufficient.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i>arXiv, 2018</i> <br> [<a href="https://arxiv.org/abs/1806.00667" target="_blank">arXiv</a>] [<a href="bibtex/GalSmith2018Sufficient.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201808_GalSmith2018Sufficient.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="BuschGal2017Automating" style="text-align: center;">
    <span class="images">
      <a href="http://cospar2018.org/wp-content/uploads/2018/06/COSPAR2018_Onsite-Program.pdf" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/astroid.jpg" title="Automating Asteroid Shape Modeling From Radar Images" alt="Automating Asteroid Shape Modeling From Radar Images" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Automating Asteroid Shape Modeling From Radar Images</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Automating Asteroid Shape Modeling From Radar Images</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Characterizing the shapes and spin states of near-Earth asteroids is essential both for trajectory predictions to rule out potential future Earth impacts and for planning spacecraft missions. But reconstructing objects’ shapes and spins from delay-Doppler data is a computationally intensive inversion problem. We implement a Bayesian optimization routine that uses SHAPE to autonomously search the space of spin-state parameters, yielding spin state constraints within a factor of 3 less computer runtime and minimal human supervision. These routines are now being incorporated into radar data processing pipelines at Arecibo.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Michael W. Busch, Agata Rozek, Sean Marshall, Grace Young, Adam Cobb, Chedy Raissi, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Lance Benner, Shantanu Naidu, Marina Brozovic, Patrick Taylor
    <br>
    <i><b>COSPAR (Committee on Space Research) Assembly, 2018</b></i> <br> [<a href="http://cospar2018.org/wp-content/uploads/2018/06/COSPAR2018_Onsite-Program.pdf" target="_blank">Program</a>] [<a href="https://adamcobb.github.io/journal/3D-Shape-Modelling-of-Asteroids.html" target="_blank">Blog Post (Adam Cobb)</a>] [BibTex]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201807_BuschGal2017Automating.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="KhanGal2018Fast" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1806.04854" target="_blank">
        <img src="images/neural_network.jpg" title="Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam" alt="Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Akash Srivastava
    <br>
    <i><b>ICML, 2018</b></i> <br> [<a href="http://proceedings.mlr.press/v80/khan18a/khan18a.pdf" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1806.04854" target="_blank">arXiv</a>] [<a href="bibtex/KhanGal2018Fast.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201806_KhanGal2018Fast.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="FarquharGal2018Differentially" style="text-align: center;">
    <span class="images">
      <a href="https://pimlai.github.io/pimlai18/#papers" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/medical.jpg" title="Differentially private continual learning" alt="Differentially private continual learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Differentially private continual learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Differentially private continual learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Catastrophic forgetting can be a significant problem for institutions that must delete historic data for privacy reasons. For example, hospitals might not be able to retain patient data permanently. But neural networks trained on recent data alone will tend to forget lessons learned on old data. We present a differentially private continual learning framework based on variational inference. We estimate the likelihood of past data given the current model using differentially private generative models of old datasets. The differentially private training has no detrimental impact on our architecture’s continual learning performance, and still outperforms the current state-of-the-art non-private continual learning.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>Privacy in Machine Learning and Artificial Intelligence workshop, ICML, 2018</b></i> <br> [<a href="http://sebastianfarquhar.com/assets/pdf/Farquhar2018b.pdf" target="_blank">Paper</a>] [<a href="bibtex/FarquharGal2018Differentially.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201806_FarquharGal2018Differentially.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="CobbGal2018Loss" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1805.03901" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/calibration.jpg" title="Loss-Calibrated Approximate Inference in Bayesian Neural Networks" alt="Loss-Calibrated Approximate Inference in Bayesian Neural Networks" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Loss-Calibrated Approximate Inference in Bayesian Neural Networks</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Loss-Calibrated Approximate Inference in Bayesian Neural Networks</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Current approaches in approximate inference for Bayesian neural networks minimise the Kullback-Leibler divergence to approximate the true posterior over the weights. However, this approximation is without knowledge of the final application, and therefore cannot guarantee optimal predictions for a given task. To make more suitable task-specific approximations, we introduce a new loss-calibrated evidence lower bound for Bayesian neural networks in the context of supervised learning, informed by Bayesian decision theory. By introducing a lower bound that depends on a utility function, we ensure that our approximation achieves higher utility than traditional methods for applications that have asymmetric utility functions. Furthermore, in using dropout inference, we highlight that our new objective is identical to that of standard dropout neural networks, with an additional utility-dependent penalty term. We demonstrate our new loss-calibrated model with an illustrative medical examp... [<a href="https://oatml.cs.ox.ac.uk/publications/201806_CobbGal2018Loss.html">full abstract</a>]</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Adam D. Cobb, Stephen J. Roberts, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>Theory of deep learning workshop, ICML, 2018</b></i> <br> [<a href="https://arxiv.org/abs/1805.03901" target="_blank">arXiv</a>] [<a href="https://github.com/AdamCobb/LCBNN" target="_blank">Code</a>] [<a href="bibtex/CobbGal2018Loss.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201806_CobbGal2018Loss.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="AlizadehGal2018Using" style="text-align: center;">
    <span class="images">
      <a href="publications.html#" target="_blank">
        <img src="images/neural_network.jpg" title="Using Pre-trained Full-Precision Models to Speed Up Training Binary Networks For Mobile Devices" alt="Using Pre-trained Full-Precision Models to Speed Up Training Binary Networks For Mobile Devices" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Using Pre-trained Full-Precision Models to Speed Up Training Binary Networks For Mobile Devices</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Using Pre-trained Full-Precision Models to Speed Up Training Binary Networks For Mobile Devices</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Binary Neural Networks (BNNs) are well-suited for deploying Deep Neural Networks (DNNs) to small embedded devices but state-of-the-art BNNs need to be trained from scratch. We show how weights from a trained full-precision model can be used to speed-up training binary networks. We show that for CIFAR-10, accuracies within 1% of the full-precision model can be achieved in just 5 epochs.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, Nicholas D. Lane, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>16th ACM International Conference on Mobile Systems (MobiSys), 2018</b></i> <br> [Abstract] [BibTex]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201806_AlizadehGal2018Using.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="FarquharGal2018Towards" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1805.09733" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/medical.jpg" title="Towards Robust Evaluations of Continual Learning" alt="Towards Robust Evaluations of Continual Learning" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Towards Robust Evaluations of Continual Learning</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Towards Robust Evaluations of Continual Learning</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Continual learning experiments used in current deep learning papers do not faithfully assess fundamental challenges of learning continually, masking weak-points of the suggested approaches instead. We study gaps in such existing evaluations, proposing essential experimental evaluations that are more representative of continual learning’s challenges, and suggest a re-prioritization of research efforts in the field. We show that current approaches fail with our new evaluations and, to analyse these failures, we propose a variational loss which unifies many existing solutions to continual learning under a Bayesian framing, as either ‘prior-focused’ or ‘likelihood-focused’. We show that while prior-focused approaches such as EWC and VCL perform well on existing evaluations, they perform dramatically worse when compared to likelihood-focused approaches on other simple tasks.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>Lifelong Learning: A Reinforcement Learning Approach workshop, ICML, 2018</b></i> <br> [<a href="https://arxiv.org/abs/1805.09733" target="_blank">arXiv</a>] [<a href="bibtex/FarquharGal2018Towards.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201805_FarquharGal2018Towards.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="SmithGal2018Understanding" style="text-align: center;">
    <span class="images">
      <a href="https://arxiv.org/abs/1803.08533" target="_blank">
        <img src="https://oatml.cs.ox.ac.uk/images/test.jpg" title="Understanding Measures of Uncertainty for Adversarial Example Detection" alt="Understanding Measures of Uncertainty for Adversarial Example Detection" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Understanding Measures of Uncertainty for Adversarial Example Detection</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Understanding Measures of Uncertainty for Adversarial Example Detection</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive entropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for MC dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using MNIST to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    <a href="members/lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>UAI, 2018</b></i> <br> [<a href="http://auai.org/uai2018/proceedings/papers/207.pdf" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1803.08533" target="_blank">arXiv</a>] [<a href="bibtex/SmithGal2018Understanding.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201803_SmithGal2018Understanding.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<div class="row" id="my_div_grid">
  <div class="2u 12u$(medium)" id="KhanGal2017Vprop" style="text-align: center;">
    <span class="images">
      <a href="http://bayesiandeeplearning.org/2017/papers/50.pdf" target="_blank">
        <img src="images/neural_network.jpg" title="Vprop: Variational Inference using RMSprop" alt="Vprop: Variational Inference using RMSprop" style="width: 100%; max-width: 100%">
      </a>
    </span>
  </div>
  <div class="9u 12u$(medium)">
    <header>
        <!-- <h4><b>Vprop: Variational Inference using RMSprop</b></h4> -->
        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Vprop: Variational Inference using RMSprop</b></p>
    </header>
    <p style="margin: 0 0 1rem 0" >
      <p>Many computationally-efficient methods for Bayesian deep learning rely on continuous optimization algorithms, but the implementation of these methods requires significant changes to existing code-bases. In this paper, we propose Vprop, a method for variational inference that can be implemented with two minor changes to the off-the-shelf RMSprop optimizer. Vprop also reduces the memory requirements of Black-Box Variational Inference by half. We derive Vprop using the conjugate-computation variational inference method, and establish its connections to Newton’s method, natural-gradient methods, and extended Kalman filters. Overall, this paper presents Vprop as a principled, computationally-efficient, and easy-to-implement method for Bayesian deep learning.</p>
</p>
    <hr style="margin: 1rem 0 1rem 0" />
    
    Mohammad Emtiyaz Khan, Zuozhu Liu, Voot Tangkaratt, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
    <br>
    <i><b>Bayesian Deep Learning workshop, NIPS, 2017</b></i> <br> [<a href="http://bayesiandeeplearning.org/2017/papers/50.pdf" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1712.01038" target="_blank">arXiv</a>] [<a href="bibtex/KhanGal2017Vprop.bib" target="_blank">BibTex</a>]
  </div>
  <div class="1u 12u$(medium)">
    <a href="https://oatml.cs.ox.ac.uk/publications/201712_KhanGal2017Vprop.html"><span class="image"><img src="images/link-to-here.png" title="Link to this publication" alt="Link to this publication" style="max-width: 15px"></span></a>
  </div>
</div>



<hr>
<h2 style="text-align: center;">
    More publications here: <a
        href="http://www.cs.ox.ac.uk/people/yarin.gal/website/publications.html" target="_blank"><i>publications</i></a>
</h2>

						</div>
					</div>
				</div>
			</section>

<!-- Contact -->
<section id="two" class="wrapper style3">
    <div class="inner">
        <header>
            <!-- <h2><a href="contact.html" id="my_headline">Contact</a></h2> -->
            <h2>Contact</h2>
            <div class="row 200%">
                <div class="6u 12u$(medium) left">
                    <b>I am located at </b> <br>
                    <a href="https://goo.gl/maps/VdQW7KmxXn3Wa5od9" target="_blank">Department of
                      Automatic Control and Systems Engineering, The University of Sheffield</a><br>
                    Sir Henry Stephenson Building<br>
                    Mappin Street<br>
                    SHEFFIELD<br>
                    S1 4DT<br>
                    UK
                </div>
                <div class="6u 12u$(medium)">
                    <b>GoogleScholar</b>: <a href="https://scholar.google.co.uk/citations?user=ZOhomxwAAAAJ&hl=en" target="_blank">Peng Wang</a><br>
                    <b>ResearchGate</b>: <a href="https://www.researchgate.net/profile/Peng_Wang116" target="_blank">Peng Wang</a><br>			
                    <b>Twitter</b>: <a href="https://twitter.com/lcccboy" target="_blank">@lccboy</a><br>
                    <b>Github</b>: <a href="https://github.com/grapesonwang" target="_blank">grapesonwang</a><br>
                    <b>Email</b>: <a href="mailto:peng.wang@sheffield.ac.uk">peng.wang@sheffield.ac.uk</a>
                </div>
            </div>
<!--            <footer class="align-center">
                <p></p>
                <br>
                <h4>Are you looking to do a PhD in machine learning? Did you do a PhD in another field and want to do a postdoc in machine learning? Would you like to visit the group?</h4>
                <a href="https://oatml.cs.ox.ac.uk/apply.html" class="button alt">How to apply</a>
            </footer>
-->
            <br>
            <br>

        </header>
    </div>
</section>


<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<ul class="icons">
						<li><a href="https://scholar.google.co.uk/citations?user=ZOhomxwAAAAJ&hl=en" target="_blank" class="icon fa-google"><span class="label">GoogleScholar</span></a></li>
						<li><a href="https://twitter.com/lcccboy" target="_blank" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/grapesonwang" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto:peng.wang@sheffield.ac.uk" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
					</ul>
				</div>
				<div class="copyright">
					&copy; Peng Wang. All rights reserved. Template was adapted from <a href="https://templated.co/" target="_blank">Templated</a> under <a href="https://creativecommons.org/licenses/by/3.0/" target="_blank">CCA 3.0</a> licence.
				</div>
			</footer>

<!-- Scripts -->
			<script src="/assets/js/jquery.min.js"></script>
			<script src="/assets/js/jquery.scrollex.min.js"></script>
			<script src="/assets/js/skel.min.js"></script>
			<script src="/assets/js/util.js"></script>
			<script src="/assets/js/main.js"></script>

</body>
</html>
