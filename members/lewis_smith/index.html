<!DOCTYPE HTML>
<html>
	<head>




	<title>Lewis Smith | OATML | Oxford Applied and Theoretical Machine Learning Group</title>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-45108170-4"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-45108170-4');
	</script>
	<script type="text/javascript" async
  		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<meta name="description" content="Lewis Smith is a DPhil student supervised by Yarin Gal. His main interests are in the reliability and robustness of machine learning algorithms, Bayesian methods, and the automatic learning of structure." />
	<meta name="keywords" content="Lewis Smith,Lewis Smith | OATML | Oxford Applied and Theoretical Machine Learning Group,OATML,Oxford,Machine learning,ML,Artificial intelligence,AI,Deep learning,DL,Bayesian deep learning,BDL,Bayesian,Bayesian modelling,Probabilistic modelling,Research group,Resarch,Fundamental Research,Pragmatic research,Yarin,Gal,Yarin Gal,yaringal" />
	<meta charset="utf-8" />
	<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
	<meta http-equiv="Pragma" content="no-cache" />
	<meta http-equiv="Expires" content="0" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<link rel="stylesheet" href="https://oatml.cs.ox.ac.uk/assets/css/main.css" />

    <link rel="apple-touch-icon-precomposed" href="https://oatml.cs.ox.ac.uk/images/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" href="https://oatml.cs.ox.ac.uk/images/apple-touch-icon-114x114.png">
    <link rel="shortcut icon" href="https://oatml.cs.ox.ac.uk/images/favicon.ico">

	<meta property="og:title" content="Lewis Smith | OATML | Oxford Applied and Theoretical Machine Learning Group" />
	<meta property="og:type" content="website" />
	<meta property="og:image" content="https://oatml.cs.ox.ac.uk/./images/member_ls.jpg" />
	<meta property="og:description" content="Lewis Smith is a DPhil student supervised by Yarin Gal. His main interests are in the reliability and robustness of machine learning algorithms, Bayesian methods, and the automatic learning of structure." />
	<style type="text/css">
		.my_headline {
			color: inherit !important;
			font-weight: inherit;
			text-decoration: inherit;
		}
		/*.my_logo {display: block !important; margin: auto; padding: 20px 0px 20px 0px; width: 50%;}*/
		.my_logo {
			display: block !important;
			margin: auto;
			padding: 20px 0px 20px 0px;
			max-height: 70%;
		}
		.banner > article .inner {height: 100%}
		.banner > article .inner > header {height: 85%}
		.banner_top {height: 15%}
		body.is-mobile .banner_top {height: 5%}
		@media screen and (max-width: 980px) {
			.banner_top {height: 10%}
			.my_logo {max-height: 50%}
		}
		.my_box_image {overflow: hidden; max-height: 50px;}
		.my_news_div {padding: 0 !important}
		.my_news_div div {margin: 0em 1em 1em 1em}
		.my_news_div div header h4 {
			font-weight: 700;
			text-transform: uppercase;
			color: #484848 !important;
			word-spacing: 3pt;
		}
		.my_news_div div header p {
			font-size: 0.9em;
			color: #aaa !important;
			margin-top: -0.6em;
			margin-bottom: 0.1em;
		}
		.my_news_span {
			padding: 0 !important;
			background-color: transparent !important;
		}
		.my_news_span img {
			border-radius: 100%;
			width: 100%;
			border: solid 0.5em rgba(144, 144, 144, 0.25);
		}
		div #my_div_grid:nth-child(2n) {
			background-color: rgba(0, 0, 0, 0.075);
		}
		div #my_div_grid {
			padding: 1rem 0rem 1rem 0rem;
			margin: 0;
		}
		.wrapper.style3.pageheader {
			background-size: 100% !important;
			background-position: top !important;
			background-image: url('https://oatml.cs.ox.ac.uk/images/bg_crop.jpg') !important;
		}
		/* Mobile screens */
		@media screen and (max-width: 980px) {
			.wrapper.style3.pageheader {background-size: 200% !important;}
			.ox-link {display: none;}
			.group-title-link {display: none;}
			.group-title-link-short {display: inline;}
			div.my_news_div div.fit {
				text-align : inherit !important;
			}
			span.my_news_span.image.right {
				float : left;
				margin: 0 1.5rem 1rem 0;
			}
		}
		/* Mobile screens */
		@media screen and (max-width: 435px) {
			.my_header {
				min-height: 120pt !important;
			}
		}
		/* Desktop screens */
		@media screen and (min-width: 980px) {
			.group-title-link-short {display: none;}
			.group-title-link {
				display: inline;
				vertical-align: bottom !important;
				border-left: 2px solid white;
				height: 500px;
				padding: 0px 0px 0px 10px;
				margin: 0px 0px 0px 10px;
			}
		}
		/* Mobile screens */
		@media screen and (max-width: 980px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_1_mobile.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford2_mobile.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal1_mobile.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_1_mobile.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_2_mobile.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford1_mobile.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal2_mobile.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_2_mobile.jpg') !important;
				}
			
		}
		/* Desktop screens */
		@media screen and (min-width: 980px) and (max-width: 1920px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_1.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford2.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal1.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_1.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_2.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford1.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal2.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_2.jpg') !important;
				}
			
		}
		/* Huge screens */
		@media screen and (min-width: 1920px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_1_huge.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford2_huge.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal1_huge.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_1_huge.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_2_huge.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford1_huge.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal2_huge.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_2_huge.jpg') !important;
				}
			
		}
		/* Mobile screens */
		@media screen and (max-height: 600px) {
			.indicators {display: none;}
		}
	</style>

	</head>
	<body>

		<!-- Header -->
					<header id="header" class="alt">
				<div class="logo">
					<img src="https://oatml.cs.ox.ac.uk/images/cs_logo.png" class="ox-link" style="height: 58px; vertical-align: top !important" alt="University of Oxford Department of Computer Science" usemap="#compscimap" />
					<map name="compscimap" id="compscimap">
                                <area shape="rect" coords="0,0,55,55" href="http://www.ox.ac.uk/" alt="University of Oxford" title="University of Oxford"/>
                                <area shape="rect" coords="65,0,170,55" href="http://www.cs.ox.ac.uk/" alt="Department of Computer Science - Home" title="Department of Computer Science - Home"/>
                    </map>
					<a href="../../index.html" class="group-title-link">
						<!-- <img src="images/logo.png" style="height: 55px" alt="Group Home"> -->
						Group Home
					</a>
					<a href="../../index.html" class="group-title-link-short">
						Home
					</a>
				</div>
				<a href="index.html#menu">Menu</a>
			</header>


		<!-- Nav -->
					<nav id="menu">
				<ul class="links">
					<li><a href="../../index.html">Home</a></li>
					<li><a href="../../news.html">News</a></li>
					<li><a href="../../publications.html">Publications</a></li>
					<li><a href="../../code.html">Reproducibility and Code</a></li>
					<li><a href="../../blog.html">Blog</a></li>
					<li><a href="../../members.html">Group Members</a></li>
					<!-- <li><a href="research.html">Research Themes</a></li> -->
					<li><a href="https://oatml.cs.ox.ac.uk/apply.html">Apply to the Group</a></li>
					<!-- <li><a href="contact.html">Contact</a></li> -->
				</ul>
			</nav>


		<!-- One -->
					<section id="One" class="wrapper style3 pageheader">
				<div class="inner">
					<header class="align-center">
						<p>Oxford Applied and Theoretical Machine Learning Group</p>
						<h1>OATML</h1>
					</header>
				</div>
			</section>

		<section id="two" class="wrapper style2">
	<div class="inner">
		<div class="box">
			<div class="content">

<header class="align-left">
    <h4><a href="../../members.html">Back to all members...</a></h4>
</header>

<header class="align-center">
	<p>Lewis Smith</p>
	<h3>PhD, started 2017</h3>
	<ul class="icons">
	
	
		<li><a href="https://github.com/lsgos" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
	
	
		<li><a href="mailto:lewis.smith@kellogg.ox.ac.uk" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
	
	
		<li><a href="http://www.robots.ox.ac.uk/~lsgs/" class="icon fa-globe" target="_blank"><span class="label">Web</span></a></li>
	
	
	</ul>
</header>

<div class="4u 12u$(small)" style="margin-left: auto; margin-right: auto;">
    <span class="image main">
        <img src="../../images/member_ls.jpg" alt="" />
    </span>
</div>


<p>
<p>Lewis Smith is a DPhil student supervised by Yarin Gal.
His main interests are in the reliability and robustness of machine
learning algorithms, Bayesian methods, and the automatic learning
of structure (such as invariances in the data). He is also a member
of the <a href="https://oatml.cs.ox.ac.uk/members/lewis_smith/www.aims.robots.ox.ac.uk">AIMS CDT</a>.  Before joining OATML,
he recieved his masters degree in physics from the University of
Manchester.</p>

</p>





<header class="align-left">
	<p style="margin-bottom: 1rem">Publications</p>
</header>
<div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Smith2020CapsulesWorkshop" style="text-align: center;">
		    <span class="images">
		      <a href="https://oolworkshop.github.io/program/ool_6.html" target="_blank">
		        <img src="../../images/capsule_parts.png" title="Capsule Networks: A Generative Probabilistic Perspective" alt="Capsule Networks: A Generative Probabilistic Perspective" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Capsule Networks: A Generative Probabilistic Perspective</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Capsule Networks: A Generative Probabilistic Perspective</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>‘Capsule’ models try to explicitly represent the poses of objects, enforcing a linear relationship between an objects pose and those of its constituent parts. This modelling assumption should lead to robustness to viewpoint changes since the object-component relationships are invariant to the poses of the object. We describe a probabilistic generative model that encodes these assumptions. Our probabilistic formulation separates the generative assumptions of the model from the inference scheme, which we derive from a variational bound. We experimentally demonstrate the applicability of our unified objective, and the use of test time optimisation to solve problems inherent to amortised inference.</p>
 -->
		      'Capsule' models try to explicitly represent the poses of objects, enforcing a linear relationship between an objects pose and those of its constituent parts. This modelling assumption should lead to robustness to viewpoint changes since the object-component relationships are invariant to the poses of the object. We describe a probabilistic generative model that encodes these assumptions. Our probabilistic formulation separates the generative assumptions of the model from the inference scheme, which we derive from a variational bound. We experimentally demonstrate the applicability of our unified objective, and the use of test time optimisation to solve problems inherent to amortised inference.
</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/lisa_schut/">Lisa Schut</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Mark van der Wilk
		    <br>
		    <i> Object Oriented Learning Workshop, ICML 2020</i> <br> [<a href="https://oolworkshop.github.io/program/ool_6.html" target="_blank">Paper</a>] <br>
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="VanAmersfoortSmithGal2020Simple" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/2003.02037" target="_blank">
		        <img src="../../images/twomoons.png" title="Uncertainty Estimation Using a Single Deep Deterministic Neural Network" alt="Uncertainty Estimation Using a Single Deep Deterministic Neural Network" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Uncertainty Estimation Using a Single Deep Deterministic Neural Network</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Uncertainty Estimation Using a Single Deep Deterministic Neural Network</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.</p>
 -->
		      We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="index.html">Lewis Smith</a>, Yee Whye Teh, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <i><b>ICML, 2020</b></i> <br> [<a href="https://arxiv.org/abs/2003.02037" target="_blank">Paper</a>] [<a href="../../bibtex/VanAmersfoortSmithGal2020Simple.bib" target="_blank">BibTex</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Farquhar2020Try" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/2002.03704" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/try_depth.png" title="Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks" alt="Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Try Depth Instead of Weight Correlations: Mean-field is a Less Restrictive Assumption for Deeper Networks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>We challenge the longstanding assumption that the mean-field approximation for variational inference in Bayesian neural networks is severely restrictive. We argue mathematically that full-covariance approximations only improve the ELBO if they improve the expected log-likelihood. We further show that deeper mean-field networks are able to express predictive distributions approximately equivalent to shallower full-covariance networks. We validate these observations empirically, demonstrating that deeper models decrease the divergence between diagonal- and full-covariance Gaussian fits to the true posterior.</p>
 -->
		      We challenge the longstanding assumption that the mean-field approximation for variational inference in Bayesian neural networks is severely restrictive. We argue mathematically that full-covariance approximations only improve the ELBO if they improve the expected log-likelihood. We further show that deeper mean-field networks are able to express predictive distributions approximately equivalent to shallower full-covariance networks. We validate these observations empirically, demonstrating that deeper models decrease the divergence between diagonal- and full-covariance Gaussian fits to the true posterior. 
</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <b><span style="color:red">Contributed talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/45.pdf" target="_blank">Workshop paper</a>], [<a href="https://arxiv.org/abs/2002.03704" target="_blank">arXiv</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Veitch-Michaelis2019Flood" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1910.03019" target="_blank">
		        <img src="../../images/esa_talk.jpg" title="Flood Detection On Low Cost Orbital Hardware" alt="Flood Detection On Low Cost Orbital Hardware" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Flood Detection On Low Cost Orbital Hardware</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Flood Detection On Low Cost Orbital Hardware</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Satellite imaging is a critical technology for monitoring and responding to natural disasters such as flooding. Despite the capabilities of modern satellites, there is still much to be desired from the perspective of first response organisations like UNICEF. Two main challenges are rapid access to data, and the ability to automatically identify flooded regions in images. We describe a prototypical flood segmentation system, identifying cloud, water and land, that could be deployed on a constellation of small satellites, performing processing on board to reduce downlink bandwidth by 2 orders of magnitude. We target PhiSat-1, part of the FSSCAT mission, which is planned to be launched by the European Space Agency (ESA) near the start of 2020 as a proof of concept for this new technology.</p>
 -->
		      Satellite imaging is a critical technology for monitoring and responding to natural disasters such as flooding. Despite the capabilities of modern satellites, there is still much to be desired from the perspective of first response organisations like UNICEF. Two main challenges are rapid access to data, and the ability to automatically identify flooded regions in images. We describe a prototypical flood segmentation system, identifying cloud, water and land, that could be deployed on a constellation of small satellites, performing processing on board to reduce downlink bandwidth by 2 orders of magnitude. We target PhiSat-1, part of the FSSCAT mission, which is planned to be launched by the European Space Agency (ESA) near the start of 2020 as a proof of concept for this new technology.
</p>
		    <hr style="margin: 1rem 0 1rem 0" />Joshua Veitch-Michaelis, Gonzalo Mateo-Garcia, Silviu Oprea, <a href="index.html">Lewis Smith</a>, Atilim Gunes Baydin, Dietmar Backes, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Guy Schumann
		    <br>
		    <b><span style="color:red">Spotlight talk</span>, <i>Artificial Intelligence for Humanitarian Assistance and Disaster Response (AI+HADR) NeurIPS 2019 Workshop</i></b> <br> [<a href="https://arxiv.org/abs/1910.03019" target="_blank">arXiv</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="OATML2019DiabeticRetinopathyDiagnosis" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1912.10481" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/diabetic_retinopathy_diagnosis.jpg" title="A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks" alt="A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods’ robustness and scalability, assessing whether new tools give ‘better’ uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on diabetic retinopathy diagnosis. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening—i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert-domain to reflect real-world use of model uncertainty in automated diagnosis. We develop multiple tasks that fall under this application, including out-of-distribution detection and robustness to distribution shift. We then perform a systematic comparison of well-tuned BDL techniques on the various tasks. From our comparison we conclude that some current techniques which solve benchmarks such as UCI `overfit’ their uncertainty to the dataset—when evaluated on our benchmark these underperform in comparison to simpler baselines. The code for the benchmark, its baselines, and a simple API for evaluating new BDL tools are made available at https://github.com/oatml/bdl-benchmarks.</p>
 -->
		      Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods' robustness and scalability, assessing whether new tools give 'better' uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on diabetic retinopathy diagnosis. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening---i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert-... [<a href="https://oatml.cs.ox.ac.uk/publications/201907_OATML2019DiabeticRetinopathyDiagnosis.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../angelos_filos/index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, Arnoud de Kroon, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <i>Preprint, 2019</i> <br> [<a href="http://www.cs.ox.ac.uk/people/angelos.filos/publications/diabetic_retinopathy_diagnosis.pdf" target="_blank">Preprint</a>] [<a href="../../bibtex/OATML2019DiabeticRetinopathyDiagnosis.bib" target="_blank">BibTex</a>] [<a href="https://github.com/OATML/bdl-benchmarks" target="_blank">Code</a>] <br> <i>arXiv, 2019</i> <br> [<a href="http://arxiv.org/abs/1912.10481" target="_blank">arXiv</a>] <br> <b><span style="color:red">Spotlight talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/12.pdf" target="_blank">Paper</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Walmsley2019Galaxy" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1905.07424" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/galaxy_zoo.png" title="Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning" alt="Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Galaxy Zoo: Probabilistic Morphology through Bayesian CNNs and Active Learning</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>We use Bayesian CNNs and a novel generative model of Galaxy Zoo volunteer responses to infer posteriors for the visual morphology of galaxies. Bayesian CNN can learn from galaxy images with uncertain labels and then, for previously unlabelled galaxies, predict the probability of each possible label. Using our posteriors, we apply the active learning strategy BALD to request volunteer responses for the subset of galaxies which, if labelled, would be most informative for training our network. By combining human and machine intelligence, Galaxy Zoo will be able to classify surveys of any conceivable scale on a timescale of weeks, providing massive and detailed morphology catalogues to support research into galaxy evolution.</p>
 -->
		      We use Bayesian CNNs and a novel generative model of Galaxy Zoo volunteer responses to infer posteriors for the visual morphology of galaxies. Bayesian CNN can learn from galaxy images with uncertain labels and then, for previously unlabelled galaxies, predict the probability of each possible label. Using our posteriors, we apply the active learning strategy BALD to request volunteer responses for the subset of galaxies which, if labelled, would be most informative for training our network. By combining human and machine intelligence, Galaxy Zoo will be able to classify surveys of any conceivable scale on a timescale of weeks, providing massive and detailed morphology catalogues to support research into galaxy evolution.
</p>
		    <hr style="margin: 1rem 0 1rem 0" />Mike Walmsley, <a href="index.html">Lewis Smith</a>, Chris Lintott, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Steven Bamford, Hugh Dickinson, Lucy Fortson, Sandor Kruk, Karen Masters, Claudia Scarlata, Brooke Simmons, Rebecca Smethurst, Darryl Wright
		    <br>
		    <b><i>Monthly Notices of the Royal Astronomical Society, 2019</i></b><br> [<a href="https://academic.oup.com/mnras/advance-article-abstract/doi/10.1093/mnras/stz2816/5583078?redirectedFrom=fulltext" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1905.07424" target="_blank">arXiv</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="GalSmith2018Sufficient" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1806.00667" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/mmnist.jpg" title="Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks" alt="Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification task with a VGG13 variant.</p>
 -->
		      We prove, under two sufficient conditions, that idealised models can have no adversarial examples. We discuss which idealised models satisfy our conditions, and show that idealised Bayesian neural networks (BNNs) satisfy these. We continue by studying near-idealised BNNs using HMC inference, demonstrating the theoretical ideas in practice. We experiment with HMC on synthetic data derived from MNIST for which we know the ground-truth image density, showing that near-perfect epistemic uncertainty correlates to density under image manifold, and that adversarial images lie off the manifold in our setting. This suggests why MC dropout, which can be seen as performing approximate inference, has been observed to be an effective defence against adversarial examples in practice; We highlight failure-cases of non-idealised BNNs relying on dropout, suggesting a new attack for dropout models and a new defence as well. Lastly, we demonstrate the defence on a cats-vs-dogs image classification ta... [<a href="https://oatml.cs.ox.ac.uk/publications/201808_GalSmith2018Sufficient.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <i>arXiv, 2018</i> <br> [<a href="https://arxiv.org/abs/1806.00667" target="_blank">arXiv</a>] [<a href="../../bibtex/GalSmith2018Sufficient.bib" target="_blank">BibTex</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="SmithGal2018Understanding" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1803.08533" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/test.jpg" title="Understanding Measures of Uncertainty for Adversarial Example Detection" alt="Understanding Measures of Uncertainty for Adversarial Example Detection" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Understanding Measures of Uncertainty for Adversarial Example Detection</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Understanding Measures of Uncertainty for Adversarial Example Detection</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive entropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for MC dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using MNIST to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.</p>
 -->
		      Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive entropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for MC dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using MNIST to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.
</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <i><b>UAI, 2018</b></i> <br> [<a href="http://auai.org/uai2018/proceedings/papers/207.pdf" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/1803.08533" target="_blank">arXiv</a>] [<a href="../../bibtex/SmithGal2018Understanding.bib" target="_blank">BibTex</a>]
		  </div>
		</div><br>
<br>
<header class="align-left">
	<p style="margin-bottom: 1rem">Reproducibility and Code</p>
</header><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="bdlb" style="text-align: center;">
		    <span class="images">
		      <a href="https://github.com/OATML/bdl-benchmarks" target="_blank">
		        <img src="../../images/BDLB3.PNG" title="Code for Bayesian Deep Learning Benchmarks" alt="Code for Bayesian Deep Learning Benchmarks" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Code for Bayesian Deep Learning Benchmarks</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Code for Bayesian Deep Learning Benchmarks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <p>In order to make real-world difference with <strong>Bayesian Deep Learning</strong> (BDL) tools, the tools must scale to real-world settings. And for that we, the research community, must be able to evaluate our inference tools (and iterate quickly) with real-world benchmark tasks. We should be able to do this without necessarily worrying about application-specific domain knowledge, like the expertise often required in medical applications for example. We require benchmarks to test for inference robustness, performance, and accuracy, in addition to cost and effort of development. These benchmarks should be at a variety of scales, ranging from toy MNIST-scale benchmarks for fast development cycles, to large data benchmarks which are truthful to real-world applications, capturing their constraints.</p>

		    </p>
		    <a href="https://github.com/OATML/bdl-benchmarks" target="_blank">Code</a><hr style="margin: 1rem 0 1rem 0" /><a href="../angelos_filos/index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		  </div>
		</div><!-- reset colours.. -->
<div class="row" id="my_div_grid" style="visibility: hidden;">
</div><br>
<br>
<header class="align-left">
	<p style="margin-bottom: 1rem">Blog Posts</p>
</header><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="are-capsules-a-good-idea-a-generative-perspective" style="text-align: center;">
		    <span class="images">
		      <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/10/are-capsules-a-good-idea-a-generative-perspective.html">
		        <img src="../../images/capsule_parts.png" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Are capsules a good idea? A generative perspective</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>I’ve recently written a paper on a fully probabilistic version of capsule networks. While trying to get this kind of model to work, I found some interesting conceptual issues with the ideas underlying capsule networks. Some of these issues are a bit philosophical in nature and I haven’t thought of a good way to pin them down in an ML conference paper. But I think they could inform research when we design new probabilistic vision models (and they are very interesting), so I’ve tried to give some insight into them here. This blog post is a companion piece to my paper: I start by introducing capsules from a generative probabilistic interpretation in a high level way, and then dive into a discussion about the conceptual issues I found. I will present the paper at the Object Oriented Learning workshop at ICML on Friday (July 17), so do drop by if you want to chat …</p>

	    	        <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/10/are-capsules-a-good-idea-a-generative-perspective.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Lewis Smith</a>, <span>10 Jul 2020</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="ICML_2020" style="text-align: center;">
		    <span class="images">
		      <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/10/ICML_2020.html">
		        <img src="https://oatml.cs.ox.ac.uk//images/icml.jpg" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>13 OATML Conference and Workshop papers at ICML 2020</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>We are glad to share the following 13 papers by OATML authors and collaborators to be presented at this ICML conference and workshops …</p>

	    	        <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/10/ICML_2020.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../angelos_filos/index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/lisa_schut/">Lisa Schut</a>, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, <a href="https://oatml.cs.ox.ac.uk/members/pascal_notin/">Pascal Notin</a>, <a href="../andreas_kirsch/index.html">Andreas Kirsch</a>, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, <a href="../joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="https://oatml.cs.ox.ac.uk/members/jishnu_mukhoti/">Jishnu Mukhoti</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>10 Jul 2020</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="NeurIPS_2019" style="text-align: center;">
		    <span class="images">
		      <a href="../../blog/2019/12/08/NeurIPS_2019.html">
		        <img src="https://oatml.cs.ox.ac.uk//images/vancouver_image.jpg" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>25 OATML Conference and Workshop papers at NeurIPS 2019</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>We are glad to share the following 25 papers by OATML authors and collaborators to be presented at this NeurIPS conference and workshops. …</p>

	    	        <a href="../../blog/2019/12/08/NeurIPS_2019.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../angelos_filos/index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, <a href="../andreas_kirsch/index.html">Andreas Kirsch</a>, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, <a href="../joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>08 Dec 2019</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="bdlb" style="text-align: center;">
		    <span class="images">
		      <a href="https://github.com/OATML/bdl-benchmarks">
		        <img src="https://oatml.cs.ox.ac.uk//images/BDLB3.PNG" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Bayesian Deep Learning Benchmarks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>In order to make real-world difference with <strong>Bayesian Deep Learning</strong> (BDL) tools, the tools must scale to real-world settings. And for that we, the research community, must be able to evaluate our inference tools (and iterate quickly) with real-world benchmark tasks. We should be able to do this without necessarily worrying about application-specific domain knowledge, like the expertise often required in medical applications for example. We require benchmarks to test for inference robustness, performance, and accuracy, in addition to cost and effort of development. These benchmarks should be at a variety of scales, ranging from toy MNIST-scale benchmarks for fast development cycles, to large data benchmarks which are truthful to real-world applications, capturing their constraints. …</p>

	    	        <a href="https://github.com/OATML/bdl-benchmarks">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../angelos_filos/index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>14 Jun 2019</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div></div>
		</div>
	</div>
</section>


		<!-- Contact -->
		<section id="two" class="wrapper style3">
    <div class="inner">
        <header>
            <!-- <h2><a href="contact.html" id="my_headline">Contact</a></h2> -->
            <h2>Contact</h2>
            <div class="row 200%">
                <div class="6u 12u$(medium) left">
                    <b>We are located at </b> <br>
                    <a href="https://goo.gl/maps/Y6xinHJ5T7o" target="_blank">Department of
                      Computer Science, University of Oxford</a><br>
                    Wolfson Building<br>
                    Parks Road<br>
                    OXFORD<br>
                    OX1 3QD<br>
                    UK
                </div>
                <div class="6u 12u$(medium)">
                    <b>Twitter</b>: <a href="https://twitter.com/OATML_Oxford" target="_blank">@OATML_Oxford</a><br>
                    <b>Github</b>: <a href="https://github.com/oatml" target="_blank">OATML</a><br>
                    <b>Email</b>: <a href="mailto:oatml@cs.ox.ac.uk">oatml@cs.ox.ac.uk</a>
                </div>
            </div>
            <footer class="align-center">
                <p></p>
                <br>
                <h4>Are you looking to do a PhD in machine learning? Did you do a PhD in another field and want to do a postdoc in machine learning? Would you like to visit the group?</h4>
                <!-- <a href="contact.html" class="button alt">How to get here</a> -->
                <a href="https://oatml.cs.ox.ac.uk/apply.html" class="button alt">How to apply</a>
            </footer>
            <br>
            <br>
        </header>
    </div>
</section>


		<!-- Footer -->
					<footer id="footer">
				<div class="container">
					<ul class="icons">
						<li><a href="https://twitter.com/OATML_Oxford" target="_blank" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/OATML" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
						<!-- <li><a href="contact.html" class="icon fa-envelope-o"><span class="label">Email</span></a></li> -->
						<li><a href="mailto:oatml@cs.ox.ac.uk" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
					</ul>
				</div>
				<div class="copyright">
					&copy; Yarin Gal. All rights reserved. Template was adapted from <a href="https://templated.co/" target="_blank">Templated</a> and some photos are used from <a href="http://commons.wikimedia.org" target="_blank">Wikimedia</a> under <a href="https://creativecommons.org/licenses/by/3.0/" target="_blank">CCA 3.0</a> licence.
				</div>
			</footer>

		<!-- Scripts -->
					<script src="https://oatml.cs.ox.ac.uk/assets/js/jquery.min.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/jquery.scrollex.min.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/skel.min.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/util.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/main.js"></script>

	</body>
</html>