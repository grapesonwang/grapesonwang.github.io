<!DOCTYPE HTML>
<html>
	<head>




	<title>Angelos Filos | OATML | Oxford Applied and Theoretical Machine Learning Group</title>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-45108170-4"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-45108170-4');
	</script>
	<script type="text/javascript" async
  		src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	<meta name="description" content="Angelos is a DPhil student in the Department of Computer Science at the University of Oxford, where he works in the Applied and Theoretical Machine Learning group (OATML) under the supervision of Yarin Gal." />
	<meta name="keywords" content="Angelos Filos,Angelos Filos | OATML | Oxford Applied and Theoretical Machine Learning Group,OATML,Oxford,Machine learning,ML,Artificial intelligence,AI,Deep learning,DL,Bayesian deep learning,BDL,Bayesian,Bayesian modelling,Probabilistic modelling,Research group,Resarch,Fundamental Research,Pragmatic research,Yarin,Gal,Yarin Gal,yaringal" />
	<meta charset="utf-8" />
	<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
	<meta http-equiv="Pragma" content="no-cache" />
	<meta http-equiv="Expires" content="0" />
	<meta name="viewport" content="width=device-width, initial-scale=1" />
	<link rel="stylesheet" href="https://oatml.cs.ox.ac.uk/assets/css/main.css" />

    <link rel="apple-touch-icon-precomposed" href="https://oatml.cs.ox.ac.uk/images/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" href="https://oatml.cs.ox.ac.uk/images/apple-touch-icon-114x114.png">
    <link rel="shortcut icon" href="https://oatml.cs.ox.ac.uk/images/favicon.ico">

	<meta property="og:title" content="Angelos Filos | OATML | Oxford Applied and Theoretical Machine Learning Group" />
	<meta property="og:type" content="website" />
	<meta property="og:image" content="https://oatml.cs.ox.ac.uk/./images/member_af.jpg" />
	<meta property="og:description" content="Angelos is a DPhil student in the Department of Computer Science at the University of Oxford, where he works in the Applied and Theoretical Machine Learning group (OATML) under the supervision of Yarin Gal." />
	<style type="text/css">
		.my_headline {
			color: inherit !important;
			font-weight: inherit;
			text-decoration: inherit;
		}
		/*.my_logo {display: block !important; margin: auto; padding: 20px 0px 20px 0px; width: 50%;}*/
		.my_logo {
			display: block !important;
			margin: auto;
			padding: 20px 0px 20px 0px;
			max-height: 70%;
		}
		.banner > article .inner {height: 100%}
		.banner > article .inner > header {height: 85%}
		.banner_top {height: 15%}
		body.is-mobile .banner_top {height: 5%}
		@media screen and (max-width: 980px) {
			.banner_top {height: 10%}
			.my_logo {max-height: 50%}
		}
		.my_box_image {overflow: hidden; max-height: 50px;}
		.my_news_div {padding: 0 !important}
		.my_news_div div {margin: 0em 1em 1em 1em}
		.my_news_div div header h4 {
			font-weight: 700;
			text-transform: uppercase;
			color: #484848 !important;
			word-spacing: 3pt;
		}
		.my_news_div div header p {
			font-size: 0.9em;
			color: #aaa !important;
			margin-top: -0.6em;
			margin-bottom: 0.1em;
		}
		.my_news_span {
			padding: 0 !important;
			background-color: transparent !important;
		}
		.my_news_span img {
			border-radius: 100%;
			width: 100%;
			border: solid 0.5em rgba(144, 144, 144, 0.25);
		}
		div #my_div_grid:nth-child(2n) {
			background-color: rgba(0, 0, 0, 0.075);
		}
		div #my_div_grid {
			padding: 1rem 0rem 1rem 0rem;
			margin: 0;
		}
		.wrapper.style3.pageheader {
			background-size: 100% !important;
			background-position: top !important;
			background-image: url('https://oatml.cs.ox.ac.uk/images/bg_crop.jpg') !important;
		}
		/* Mobile screens */
		@media screen and (max-width: 980px) {
			.wrapper.style3.pageheader {background-size: 200% !important;}
			.ox-link {display: none;}
			.group-title-link {display: none;}
			.group-title-link-short {display: inline;}
			div.my_news_div div.fit {
				text-align : inherit !important;
			}
			span.my_news_span.image.right {
				float : left;
				margin: 0 1.5rem 1rem 0;
			}
		}
		/* Mobile screens */
		@media screen and (max-width: 435px) {
			.my_header {
				min-height: 120pt !important;
			}
		}
		/* Desktop screens */
		@media screen and (min-width: 980px) {
			.group-title-link-short {display: none;}
			.group-title-link {
				display: inline;
				vertical-align: bottom !important;
				border-left: 2px solid white;
				height: 500px;
				padding: 0px 0px 0px 10px;
				margin: 0px 0px 0px 10px;
			}
		}
		/* Mobile screens */
		@media screen and (max-width: 980px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_1_mobile.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford2_mobile.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal1_mobile.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_1_mobile.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_2_mobile.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford1_mobile.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal2_mobile.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_2_mobile.jpg') !important;
				}
			
		}
		/* Desktop screens */
		@media screen and (min-width: 980px) and (max-width: 1920px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_1.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford2.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal1.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_1.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_2.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford1.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal2.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_2.jpg') !important;
				}
			
		}
		/* Huge screens */
		@media screen and (min-width: 1920px) {
			
				#banner_image_banner_uncertainty_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_1_huge.jpg') !important;
				}
			
				#banner_image_banner_oxford2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford2_huge.jpg') !important;
				}
			
				#banner_image_banner_oatmeal1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal1_huge.jpg') !important;
				}
			
				#banner_image_banner_oats_1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_1_huge.jpg') !important;
				}
			
				#banner_image_banner_uncertainty_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_uncertainty_2_huge.jpg') !important;
				}
			
				#banner_image_banner_oxford1 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oxford1_huge.jpg') !important;
				}
			
				#banner_image_banner_oatmeal2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oatmeal2_huge.jpg') !important;
				}
			
				#banner_image_banner_oats_2 {
					background-image: url('https://oatml.cs.ox.ac.uk/images/banner/banner_oats_2_huge.jpg') !important;
				}
			
		}
		/* Mobile screens */
		@media screen and (max-height: 600px) {
			.indicators {display: none;}
		}
	</style>

	</head>
	<body>

		<!-- Header -->
					<header id="header" class="alt">
				<div class="logo">
					<img src="https://oatml.cs.ox.ac.uk/images/cs_logo.png" class="ox-link" style="height: 58px; vertical-align: top !important" alt="University of Oxford Department of Computer Science" usemap="#compscimap" />
					<map name="compscimap" id="compscimap">
                                <area shape="rect" coords="0,0,55,55" href="http://www.ox.ac.uk/" alt="University of Oxford" title="University of Oxford"/>
                                <area shape="rect" coords="65,0,170,55" href="http://www.cs.ox.ac.uk/" alt="Department of Computer Science - Home" title="Department of Computer Science - Home"/>
                    </map>
					<a href="../../index.html" class="group-title-link">
						<!-- <img src="images/logo.png" style="height: 55px" alt="Group Home"> -->
						Group Home
					</a>
					<a href="../../index.html" class="group-title-link-short">
						Home
					</a>
				</div>
				<a href="index.html#menu">Menu</a>
			</header>


		<!-- Nav -->
					<nav id="menu">
				<ul class="links">
					<li><a href="../../index.html">Home</a></li>
					<li><a href="../../news.html">News</a></li>
					<li><a href="../../publications.html">Publications</a></li>
					<li><a href="../../code.html">Reproducibility and Code</a></li>
					<li><a href="../../blog.html">Blog</a></li>
					<li><a href="../../members.html">Group Members</a></li>
					<!-- <li><a href="research.html">Research Themes</a></li> -->
					<li><a href="https://oatml.cs.ox.ac.uk/apply.html">Apply to the Group</a></li>
					<!-- <li><a href="contact.html">Contact</a></li> -->
				</ul>
			</nav>


		<!-- One -->
					<section id="One" class="wrapper style3 pageheader">
				<div class="inner">
					<header class="align-center">
						<p>Oxford Applied and Theoretical Machine Learning Group</p>
						<h1>OATML</h1>
					</header>
				</div>
			</section>

		<section id="two" class="wrapper style2">
	<div class="inner">
		<div class="box">
			<div class="content">

<header class="align-left">
    <h4><a href="../../members.html">Back to all members...</a></h4>
</header>

<header class="align-center">
	<p>Angelos Filos</p>
	<h3>PhD, started 2018</h3>
	<ul class="icons">
	
		<li><a href="https://twitter.com/filangelos" target="_blank" class="icon fa-twitter"><span class="label"></span></a></li>
	
	
		<li><a href="https://github.com/filangel" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
	
	
		<li><a href="mailto:angelos.filos@cs.ox.ac.uk" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
	
	
		<li><a href="https://filangel.github.io/website/" class="icon fa-globe" target="_blank"><span class="label">Web</span></a></li>
	
	
		<li><a href="https://scholar.google.co.uk/citations?user=SGjYdrEAAAAJ&hl=en&oi=ao" class="ai ai-google-scholar-square ai-2x" style="text-decoration: none;" target="_blank"><span class="label"></span></a></li>
	
	</ul>
</header>

<div class="4u 12u$(small)" style="margin-left: auto; margin-right: auto;">
    <span class="image main">
        <img src="../../images/member_af.jpg" alt="" />
    </span>
</div>


<p>
<p>Angelos is a DPhil student in the <a href="http://www.cs.ox.ac.uk">Department of Computer Science</a> at the University of Oxford, where he works in the Applied and Theoretical Machine Learning group (<a href="http://oatml.cs.ox.ac.uk">OATML</a>) under the supervision of <a href="http://www.cs.ox.ac.uk/people/yarin.gal/website">Yarin Gal</a>.
His research interests span multi-agent systems, meta-learning and reinforcement learning.
He obtained an undergraduate and master’s degree from the <a href="https://www.imperial.ac.uk/electrical-engineering">Department of Electrical and Electronic Engineering</a> at Imperial College London.
He also contracts with <a href="https://www.jpmorgan.com/global/technology/artificial-intelligence">J.P. Morgan Artificial Intelligence Research</a> group, working on generative models, distributional reinforcement learning and inverse reinforcement learning.</p>

</p>





<header class="align-left">
	<p style="margin-bottom: 1rem">Publications</p>
</header>
<div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Zhang2020Invariant" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/2003.06016" target="_blank">
		        <img src="../../images/virel.jpg" title="Invariant Causal Prediction for Block MDPs" alt="Invariant Causal Prediction for Block MDPs" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Invariant Causal Prediction for Block MDPs</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Invariant Causal Prediction for Block MDPs</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Generalization across environments is critical to the successful application of reinforcement learning algorithms to real-world challenges. In this paper, we consider the problem of learning abstractions that generalize in block MDPs, families of environments with a shared latent state space and dynamics structure over that latent space, but varying observations. We leverage tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting. We prove that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return. We further provide more general bounds on model error and generalization error in the multi-environment setting, in the process showing a connection between causal variable selection and the state abstraction framework for MDPs. We give empirical evidence that our methods work in both linear and nonlinear settings, attaining improved generalization over single- and multi-task baselines.</p>
 -->
		      Generalization across environments is critical to the successful application of reinforcement learning algorithms to real-world challenges. In this paper, we consider the problem of learning abstractions that generalize in block MDPs, families of environments with a shared latent state space and dynamics structure over that latent space, but varying observations. We leverage tools from causal inference to propose a method of invariant prediction to learn model-irrelevance state abstractions (MISA) that generalize to novel observations in the multi-environment setting. We prove that for certain classes of environments, this approach outputs with high probability a state abstraction corresponding to the causal feature set with respect to the return. We further provide more general bounds on model error and generalization error in the multi-environment setting, in the process showing a connection between causal variable selection and the state abstraction framework for MDPs. We give e... [<a href="https://oatml.cs.ox.ac.uk/publications/202006_Zhang2020Invariant.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" />Amy Zhang, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, Shagun Sodhani, <a href="index.html">Angelos Filos</a>, Marta Kwiatkowska, Joelle Pineau, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Doina Precup
		    <br>
		    <i>Causal Learning for Decision Making Workshop at ICLR, 2020</i> <br> [<a href="https://causalrlworkshop.github.io/pdf/CLDM_17.pdf" target="_blank">Paper</a>] <br> <b><i>ICML, 2020</i></b> <br> [<a href="https://arxiv.org/abs/2003.06016" target="_blank">Paper</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Filos2020Can" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/2006.14911" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/filos2020can.png" title="Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?" alt="Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called <em>robust imitative planning</em> (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model’s uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term <em>adaptive robust imitative planning</em> (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes <em>prediction</em> challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess <em>control</em>, we introduce an autonomous car novel-scene benchmark, CARNOVEL, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.</p>
 -->
		      Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called _robust imitative planning_ (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term _adaptive robust imitative planning_ (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes _prediction_ challenge, but since no benchmark evaluating OOD d... [<a href="https://oatml.cs.ox.ac.uk/publications/202006_Filos2020Can.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <b><i>ICML, 2020</i></b> <br> [<a href="https://arxiv.org/abs/2006.14911" target="_blank">Paper</a>] [<a href="https://github.com/OATML/carsuite" target="_blank">Code</a>] [<a href="https://sites.google.com/view/av-detect-recover-adapt" target="_blank">Website</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Mehta2020Uncertainty" style="text-align: center;">
		    <span class="images">
		      <a href="https://openreview.net/forum?id=H-PvDNIex" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/medical_2.jpg" title="Uncertainty Evaluation Metric for Brain Tumour Segmentation" alt="Uncertainty Evaluation Metric for Brain Tumour Segmentation" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Uncertainty Evaluation Metric for Brain Tumour Segmentation</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Uncertainty Evaluation Metric for Brain Tumour Segmentation</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>In this paper, we develop a metric designed to assess and rank uncertainty measures for the task of brain tumour sub-tissue segmentation in the BraTS 2019 sub-challenge on uncertainty quantification. The metric is designed to: (1) reward uncertainty measures where high confidence is assigned to correct assertions, and where incorrect assertions are assigned low confidence and (2) penalize measures that have higher percentages of under-confident correct assertions. Here, the workings of the components of the metric are explored based on a number of popular uncertainty measures evaluated on the BraTS 2019 dataset.</p>
 -->
		      In this paper, we develop a metric designed to assess and rank uncertainty measures for the task of brain tumour sub-tissue segmentation in the BraTS 2019 sub-challenge on uncertainty quantification. The metric is designed to: (1) reward uncertainty measures where high confidence is assigned to correct assertions, and where incorrect assertions are assigned low confidence and (2) penalize measures that have higher percentages of under-confident correct assertions. Here, the workings of the components of the metric are explored based on a number of popular uncertainty measures evaluated on the BraTS 2019 dataset.</p>
		    <hr style="margin: 1rem 0 1rem 0" />Raghav Mehta, <a href="index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Tal Arbel
		    <br>
		    <b><i>MIDL, 2020</i></b> <br> [<a href="https://openreview.net/forum?id=H-PvDNIex" target="_blank">Paper</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Tigas2019Robust" style="text-align: center;">
		    <span class="images">
		      <a href="https://ml4ad.github.io/files/papers/Robust%20Imitative%20Planning:%20Planning%20from%20Demonstrations%20Under%20Uncertainty.pdf" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/cv_uncertainty.jpg" title="Robust Imitative Planning: Planning from Demonstrations Under Uncertainty" alt="Robust Imitative Planning: Planning from Demonstrations Under Uncertainty" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Robust Imitative Planning: Planning from Demonstrations Under Uncertainty</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Robust Imitative Planning: Planning from Demonstrations Under Uncertainty</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Learning from expert demonstrations is an attractive framework for sequential decision-making in safety-critical domains such as autonomous driving, where trial and error learning has no safety guarantees during training. However, naïve use of imitation learning can fail by extrapolating incorrectly to unfamiliar situations, resulting in arbitrary model outputs and dangerous outcomes. This is especially true for high capacity parametric models such as deep neural networks, for processing high-dimensional observations from cameras or LIDAR. Instead, we model expert behaviour with a model able to capture uncertainty about previously unseen scenarios, as well as inherent stochasticity in expert demonstrations. We propose a framework for planning under epistemic uncertainty and also provide a practical realisation, called robust imitative planning (RIP), using an ensemble of deep neural density estimators. We demonstrate online robustness to out-of-training distribution scenarios on the CARLA autonomous driving simulator, improving over other probabilistic imitation learning models and reducing the total number of hazardous events while improving runtime to real-time using a trajectory library.</p>
 -->
		      Learning from expert demonstrations is an attractive framework for sequential decision-making in safety-critical domains such as autonomous driving, where trial and error learning has no safety guarantees during training. However, naïve use of imitation learning can fail by extrapolating incorrectly to unfamiliar situations, resulting in arbitrary model outputs and dangerous outcomes. This is especially true for high capacity parametric models such as deep neural networks, for processing high-dimensional observations from cameras or LIDAR. Instead, we model expert behaviour with a model able to capture uncertainty about previously unseen scenarios, as well as inherent stochasticity in expert demonstrations. We propose a framework for planning under epistemic uncertainty and also provide a practical realisation, called robust imitative planning (RIP), using an ensemble of deep neural density estimators. We demonstrate online robustness to out-of-training distribution scenarios on th... [<a href="https://oatml.cs.ox.ac.uk/publications/201912_Tigas2019Robust.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, <a href="index.html">Angelos Filos</a>, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <b><i>NeurIPS2019 Workshop on Machine Learning for Autonomous Driving</i></b> <br> [<a href="https://ml4ad.github.io/files/papers/Robust%20Imitative%20Planning:%20Planning%20from%20Demonstrations%20Under%20Uncertainty.pdf" target="_blank">Paper</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Krishnan2019Improving" style="text-align: center;">
		    <span class="images">
		      <a href="http://bayesiandeeplearning.org/2019/papers/94.pdf" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/uncertainty_small.jpg" title="Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis" alt="Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Improving MFVI in Bayesian Neural Networks with Empirical Bayes: a Study with Diabetic Retinopathy Diagnosis</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Specifying meaningful weight priors for variational inference in Bayesian deep
neural network (DNN) is a challenging problem, particularly for scaling to larger
models involving high dimensional weight space. We evaluate the recently proposed, MOdel Priors with Empirical Bayes using DNN (MOPED) method for
Bayesian DNNs within the Bayesian Deep Learning (BDL) benchmarking framework. MOPED enables scalable VI in large models by providing a way to choose
informed prior and approximate posterior distributions for Bayesian neural network
weights using Empirical Bayes framework. We benchmark MOPED with mean
field variational inference on a real-world diabetic retinopathy diagnosis task and
compare with state-of-the-art BDL techniques. We demonstrate MOPED method
provides reliable uncertainty estimates while outperforming state-of-the-art methods, offering a new strong baseline for the BDL community to compare on complex
real-world tasks involving larger models.</p>
 -->
		      Specifying meaningful weight priors for variational inference in Bayesian deep
neural network (DNN) is a challenging problem, particularly for scaling to larger
models involving high dimensional weight space. We evaluate the recently proposed, MOdel Priors with Empirical Bayes using DNN (MOPED) method for
Bayesian DNNs within the Bayesian Deep Learning (BDL) benchmarking framework. MOPED enables scalable VI in large models by providing a way to choose
informed prior and approximate posterior distributions for Bayesian neural network
weights using Empirical Bayes framework. We benchmark MOPED with mean
field variational inference on a real-world diabetic retinopathy diagnosis task and
compare with state-of-the-art BDL techniques. We demonstrate MOPED method
provides reliable uncertainty estimates while outperforming state-of-the-art methods, offering a new strong baseline for the BDL community to compare on complex
real-world tasks involving larger models.</p>
		    <hr style="margin: 1rem 0 1rem 0" />Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, <a href="index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <b><i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/94.pdf" target="_blank">Paper</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="OATML2019DiabeticRetinopathyDiagnosis" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1912.10481" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/diabetic_retinopathy_diagnosis.jpg" title="A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks" alt="A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>A Systematic Comparison of Bayesian Deep Learning Robustness in Diabetic Retinopathy Tasks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods’ robustness and scalability, assessing whether new tools give ‘better’ uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on diabetic retinopathy diagnosis. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening—i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert-domain to reflect real-world use of model uncertainty in automated diagnosis. We develop multiple tasks that fall under this application, including out-of-distribution detection and robustness to distribution shift. We then perform a systematic comparison of well-tuned BDL techniques on the various tasks. From our comparison we conclude that some current techniques which solve benchmarks such as UCI `overfit’ their uncertainty to the dataset—when evaluated on our benchmark these underperform in comparison to simpler baselines. The code for the benchmark, its baselines, and a simple API for evaluating new BDL tools are made available at https://github.com/oatml/bdl-benchmarks.</p>
 -->
		      Evaluation of Bayesian deep learning (BDL) methods is challenging. We often seek to evaluate the methods' robustness and scalability, assessing whether new tools give 'better' uncertainty estimates than old ones. These evaluations are paramount for practitioners when choosing BDL tools on-top of which they build their applications. Current popular evaluations of BDL methods, such as the UCI experiments, are lacking: Methods that excel with these experiments often fail when used in application such as medical or automotive, suggesting a pertinent need for new benchmarks in the field. We propose a new BDL benchmark with a diverse set of tasks, inspired by a real-world medical imaging application on diabetic retinopathy diagnosis. Visual inputs (512x512 RGB images of retinas) are considered, where model uncertainty is used for medical pre-screening---i.e. to refer patients to an expert when model diagnosis is uncertain. Methods are then ranked according to metrics derived from expert-... [<a href="https://oatml.cs.ox.ac.uk/publications/201907_OATML2019DiabeticRetinopathyDiagnosis.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="../lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, Arnoud de Kroon, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <i>Preprint, 2019</i> <br> [<a href="http://www.cs.ox.ac.uk/people/angelos.filos/publications/diabetic_retinopathy_diagnosis.pdf" target="_blank">Preprint</a>] [<a href="../../bibtex/OATML2019DiabeticRetinopathyDiagnosis.bib" target="_blank">BibTex</a>] [<a href="https://github.com/OATML/bdl-benchmarks" target="_blank">Code</a>] <br> <i>arXiv, 2019</i> <br> [<a href="http://arxiv.org/abs/1912.10481" target="_blank">arXiv</a>] <br> <b><span style="color:red">Spotlight talk</span>, <i>Workshop on Bayesian Deep Learning, NeurIPS 2019</i></b> <br> [<a href="http://bayesiandeeplearning.org/2019/papers/12.pdf" target="_blank">Paper</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="RoaVicensChtourouFilosRulIanGalSilva2019MALICML" style="text-align: center;">
		    <span class="images">
		      <a href="https://arxiv.org/abs/1906.04813" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/RoaVicensChtourouFilosRulIanGalSilva2019MALICML.jpg" title="Towards Inverse Reinforcement Learning for Limit Order Book Dynamics" alt="Towards Inverse Reinforcement Learning for Limit Order Book Dynamics" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Towards Inverse Reinforcement Learning for Limit Order Book Dynamics</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Towards Inverse Reinforcement Learning for Limit Order Book Dynamics</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>We investigate whether Inverse Reinforcement Learning (IRL) can infer rewards from agents within real financial stochastic environments: limit order books (LOB). Our results illustrate that complex behaviours, induced by non-linear reward functions amid agent-based stochastic scenarios, can be deduced through inference, encouraging the use of inverse reinforcement learning for opponent-modelling in multi-agent systems.</p>
 -->
		      We investigate whether Inverse Reinforcement Learning (IRL) can infer rewards from agents within real financial stochastic environments: limit order books (LOB). Our results illustrate that complex behaviours, induced by non-linear reward functions amid agent-based stochastic scenarios, can be deduced through inference, encouraging the use of inverse reinforcement learning for opponent-modelling in multi-agent systems.
</p>
		    <hr style="margin: 1rem 0 1rem 0" />Jacobo Roa-Vicens, Cyrine Chtourou, <a href="index.html">Angelos Filos</a>, Francisco Rullan, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, Ricardo Silva
		    <br>
		    <i><b>Oral Presentation, Multi-Agent Learning Workshop at the 36th International Conference on Machine Learning, 2019</b></i> <br> [<a href="https://arxiv.org/abs/1906.04813" target="_blank">arXiv</a>] [<a href="../../bibtex/RoaVicensChtourouFilosRulIanGalSilva2019MALICML.bib" target="_blank">BibTex</a>]
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="Kenton2019Generalization" style="text-align: center;">
		    <span class="images">
		      <a href="https://zackenton.github.io/files/Generalizing_Limited_Environments.pdf" target="_blank">
		        <img src="https://oatml.cs.ox.ac.uk/images/gridworld.jpg" title="Generalizing from a few environments in safety-critical reinforcement learning" alt="Generalizing from a few environments in safety-critical reinforcement learning" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Generalizing from a few environments in safety-critical reinforcement learning</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Generalizing from a few environments in safety-critical reinforcement learning</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <!-- <p>Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training (e.g. many simulated environments), allowing them to learn about every possible danger. But this is often impractical: simulations may fail to capture the full range of situations and may differ subtly from reality. This paper investigates generalizing from a limited number of training environments in deep reinforcement learning. Our experiments test whether agents can perform safely in novel environments, given varying numbers of environments at train time. Using a gridworld setting, we find that standard deep RL agents do not reliably avoid catastrophes on unseen environments – even after performing near optimally on 1000 training environments. However, we show that catastrophes can be significantly reduced (but not eliminated) with simple modifications, including Q-network ensembling to represent uncertainty and the use of a classifier trained to recognize dangerous actions.</p>
 -->
		      Before deploying autonomous agents in the real world, we need to be confident they will perform safely in novel situations. Ideally, we would expose agents to a very wide range of situations during training (e.g. many simulated environments), allowing them to learn about every possible danger. But this is often impractical: simulations may fail to capture the full range of situations and may differ subtly from reality. This paper investigates generalizing from a limited number of training environments in deep reinforcement learning. Our experiments test whether agents can perform safely in novel environments, given varying numbers of environments at train time. Using a gridworld setting, we find that standard deep RL agents do not reliably avoid catastrophes on unseen environments – even after performing near optimally on 1000 training environments. However, we show that catastrophes can be significantly reduced (but not eliminated) with simple modifications, including Q-network en... [<a href="https://oatml.cs.ox.ac.uk/publications/201903_Kenton2019Generalization.html">full abstract</a>]</p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="index.html">Angelos Filos</a>, Owain Evans, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		    <br>
		    <i><b>ICLR 2019 Workshop on Safe Machine Learning</b></i> <br> [<a href="https://zackenton.github.io/files/Generalizing_Limited_Environments.pdf" target="_blank">paper</a>]
		  </div>
		</div><br>
<br>
<header class="align-left">
	<p style="margin-bottom: 1rem">Reproducibility and Code</p>
</header><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="oatomobile" style="text-align: center;">
		    <span class="images">
		      <a href="https://github.com/OATML/oatomobile" target="_blank">
		        <img src="../../images/carla.jpg" title="OATomobile: A research framework for autonomous driving" alt="OATomobile: A research framework for autonomous driving" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>OATomobile: A research framework for autonomous driving</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>OATomobile: A research framework for autonomous driving</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <p>OATomobile is a library for autonomous driving research. OATomobile strives to expose simple, efficient, well-tuned and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research.</p>

		    </p>
		    <a href="https://github.com/OATML/oatomobile" target="_blank">Code</a><hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="bdlb" style="text-align: center;">
		    <span class="images">
		      <a href="https://github.com/OATML/bdl-benchmarks" target="_blank">
		        <img src="../../images/BDLB3.PNG" title="Code for Bayesian Deep Learning Benchmarks" alt="Code for Bayesian Deep Learning Benchmarks" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b>Code for Bayesian Deep Learning Benchmarks</b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Code for Bayesian Deep Learning Benchmarks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" >
		      <p>In order to make real-world difference with <strong>Bayesian Deep Learning</strong> (BDL) tools, the tools must scale to real-world settings. And for that we, the research community, must be able to evaluate our inference tools (and iterate quickly) with real-world benchmark tasks. We should be able to do this without necessarily worrying about application-specific domain knowledge, like the expertise often required in medical applications for example. We require benchmarks to test for inference robustness, performance, and accuracy, in addition to cost and effort of development. These benchmarks should be at a variety of scales, ranging from toy MNIST-scale benchmarks for fast development cycles, to large data benchmarks which are truthful to real-world applications, capturing their constraints.</p>

		    </p>
		    <a href="https://github.com/OATML/bdl-benchmarks" target="_blank">Code</a><hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="../lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>
		  </div>
		</div><!-- reset colours.. -->
<div class="row" id="my_div_grid" style="visibility: hidden;">
</div><br>
<br>
<header class="align-left">
	<p style="margin-bottom: 1rem">Blog Posts</p>
</header><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="ICML_2020" style="text-align: center;">
		    <span class="images">
		      <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/10/ICML_2020.html">
		        <img src="https://oatml.cs.ox.ac.uk//images/icml.jpg" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>13 OATML Conference and Workshop papers at ICML 2020</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>We are glad to share the following 13 papers by OATML authors and collaborators to be presented at this ICML conference and workshops …</p>

	    	        <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/10/ICML_2020.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/lisa_schut/">Lisa Schut</a>, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, <a href="https://oatml.cs.ox.ac.uk/members/pascal_notin/">Pascal Notin</a>, <a href="../andreas_kirsch/index.html">Andreas Kirsch</a>, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, <a href="../joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="https://oatml.cs.ox.ac.uk/members/jishnu_mukhoti/">Jishnu Mukhoti</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>10 Jul 2020</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="can_autonomous_vehicles_recover_from_ood" style="text-align: center;">
		    <span class="images">
		      <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/09/can_autonomous_vehicles_recover_from_ood.html">
		        <img src="https://oatml.cs.ox.ac.uk//blog/2020/07/09/assets/rip_animation.gif" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>In autonomous driving, we generally train models on diverse data to maximize the coverage of possible situations the vehicle may encounter at deployment. Global data coverage would be ideal, but impossible to collect, necessitating methods that can generalize safely to new scenarios. As human drivers, we do not need to re-learn how to drive in every city, even though every city is unique. Hence, we’d like a system trained in Pittsburgh and Los Angeles to also be safe when deployed in New York, where the landscape and behaviours of the drivers is different.
 …</p>

	    	        <a href="https://oatml.cs.ox.ac.uk/blog/2020/07/09/can_autonomous_vehicles_recover_from_ood.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>09 Jul 2020</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="NeurIPS_2019" style="text-align: center;">
		    <span class="images">
		      <a href="../../blog/2019/12/08/NeurIPS_2019.html">
		        <img src="https://oatml.cs.ox.ac.uk//images/vancouver_image.jpg" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>25 OATML Conference and Workshop papers at NeurIPS 2019</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>We are glad to share the following 25 papers by OATML authors and collaborators to be presented at this NeurIPS conference and workshops. …</p>

	    	        <a href="../../blog/2019/12/08/NeurIPS_2019.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="../lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, <a href="https://oatml.cs.ox.ac.uk/members/tom_rainforth/">Tom Rainforth</a>, <a href="https://oatml.cs.ox.ac.uk/members/panagiotis_tigas/">Panagiotis Tigas</a>, <a href="../andreas_kirsch/index.html">Andreas Kirsch</a>, <a href="https://oatml.cs.ox.ac.uk/members/clare_lyle/">Clare Lyle</a>, <a href="../joost_van_amersfoort/index.html">Joost van Amersfoort</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>08 Dec 2019</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="poor_generalization_in_rl" style="text-align: center;">
		    <span class="images">
		      <a href="https://oatml.cs.ox.ac.uk/blog/2019/07/02/poor_generalization_in_rl.html">
		        <img src="https://oatml.cs.ox.ac.uk//blog/2019/07/02/assets/reveal.gif" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Poor generalization can be dangerous in RL!</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>We want to develop reinforcement learning (RL) agents that can be trusted to act in high-stakes situations in the
real world. That means we need to generalize about common dangers that we might have experienced before, but in an unseen
setting. For example, we know it is dangerous to touch a hot oven, even if it’s in a room we haven’t been in
before.
 …</p>

	    	        <a href="https://oatml.cs.ox.ac.uk/blog/2019/07/02/poor_generalization_in_rl.html">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="index.html">Angelos Filos</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>02 Jul 2019</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div><div class="row" id="my_div_grid">
		  <div class="2u 12u$(medium)" id="bdlb" style="text-align: center;">
		    <span class="images">
		      <a href="https://github.com/OATML/bdl-benchmarks">
		        <img src="https://oatml.cs.ox.ac.uk//images/BDLB3.PNG" style="width: 100%; max-width: 100%">
		      </a>
		    </span>
		  </div>
		  <div class="9u 12u$(medium)">
		    <header>
		        <!-- <h4><b></b></h4> -->
		        <p style="font-size: 1rem; text-transform: none; letter-spacing: 0.07rem"><b>Bayesian Deep Learning Benchmarks</b></p>
		    </header>
		    <p style="margin: 0 0 1rem 0" ><p>In order to make real-world difference with <strong>Bayesian Deep Learning</strong> (BDL) tools, the tools must scale to real-world settings. And for that we, the research community, must be able to evaluate our inference tools (and iterate quickly) with real-world benchmark tasks. We should be able to do this without necessarily worrying about application-specific domain knowledge, like the expertise often required in medical applications for example. We require benchmarks to test for inference robustness, performance, and accuracy, in addition to cost and effort of development. These benchmarks should be at a variety of scales, ranging from toy MNIST-scale benchmarks for fast development cycles, to large data benchmarks which are truthful to real-world applications, capturing their constraints. …</p>

	    	        <a href="https://github.com/OATML/bdl-benchmarks">Full post...</a></p>
		    <hr style="margin: 1rem 0 1rem 0" /><a href="index.html">Angelos Filos</a>, <a href="../sebastian_farquhar/index.html">Sebastian Farquhar</a>, <a href="https://oatml.cs.ox.ac.uk/members/aidan_gomez/">Aidan Gomez</a>, <a href="../tim_rudner/index.html">Tim G. J. Rudner</a>, <a href="../zac_kenton/index.html">Zac Kenton</a>, <a href="../lewis_smith/index.html">Lewis Smith</a>, <a href="https://oatml.cs.ox.ac.uk/members/milad_alizadeh/">Milad Alizadeh</a>, <a href="https://oatml.cs.ox.ac.uk/members/yarin/">Yarin Gal</a>, <span>14 Jun 2019</span></div>
		  <div class="1u 12u$(medium)">
		  </div>
		</div></div>
		</div>
	</div>
</section>


		<!-- Contact -->
		<section id="two" class="wrapper style3">
    <div class="inner">
        <header>
            <!-- <h2><a href="contact.html" id="my_headline">Contact</a></h2> -->
            <h2>Contact</h2>
            <div class="row 200%">
                <div class="6u 12u$(medium) left">
                    <b>We are located at </b> <br>
                    <a href="https://goo.gl/maps/Y6xinHJ5T7o" target="_blank">Department of
                      Computer Science, University of Oxford</a><br>
                    Wolfson Building<br>
                    Parks Road<br>
                    OXFORD<br>
                    OX1 3QD<br>
                    UK
                </div>
                <div class="6u 12u$(medium)">
                    <b>Twitter</b>: <a href="https://twitter.com/OATML_Oxford" target="_blank">@OATML_Oxford</a><br>
                    <b>Github</b>: <a href="https://github.com/oatml" target="_blank">OATML</a><br>
                    <b>Email</b>: <a href="mailto:oatml@cs.ox.ac.uk">oatml@cs.ox.ac.uk</a>
                </div>
            </div>
            <footer class="align-center">
                <p></p>
                <br>
                <h4>Are you looking to do a PhD in machine learning? Did you do a PhD in another field and want to do a postdoc in machine learning? Would you like to visit the group?</h4>
                <!-- <a href="contact.html" class="button alt">How to get here</a> -->
                <a href="https://oatml.cs.ox.ac.uk/apply.html" class="button alt">How to apply</a>
            </footer>
            <br>
            <br>
        </header>
    </div>
</section>


		<!-- Footer -->
					<footer id="footer">
				<div class="container">
					<ul class="icons">
						<li><a href="https://twitter.com/OATML_Oxford" target="_blank" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://github.com/OATML" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
						<!-- <li><a href="contact.html" class="icon fa-envelope-o"><span class="label">Email</span></a></li> -->
						<li><a href="mailto:oatml@cs.ox.ac.uk" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
					</ul>
				</div>
				<div class="copyright">
					&copy; Yarin Gal. All rights reserved. Template was adapted from <a href="https://templated.co/" target="_blank">Templated</a> and some photos are used from <a href="http://commons.wikimedia.org" target="_blank">Wikimedia</a> under <a href="https://creativecommons.org/licenses/by/3.0/" target="_blank">CCA 3.0</a> licence.
				</div>
			</footer>

		<!-- Scripts -->
					<script src="https://oatml.cs.ox.ac.uk/assets/js/jquery.min.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/jquery.scrollex.min.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/skel.min.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/util.js"></script>
			<script src="https://oatml.cs.ox.ac.uk/assets/js/main.js"></script>

	</body>
</html>